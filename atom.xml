<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ymmy</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://anery.github.io/"/>
  <updated>2020-02-25T12:40:11.491Z</updated>
  <id>http://anery.github.io/</id>
  
  <author>
    <name>Yue Yuan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>NLP词向量发展历程</title>
    <link href="http://anery.github.io/posts/a71e8abb.html"/>
    <id>http://anery.github.io/posts/a71e8abb.html</id>
    <published>2020-02-25T09:08:52.000Z</published>
    <updated>2020-02-25T12:40:11.491Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章记录词向量的发展历程，包括<strong>tf-idf、word2vec、GloVe、ELMo、OpenAI GPT</strong>以及<strong>Bert</strong>，只记录个人认为比较核心的内容，以及一些值得思考的边角细节。</p><a id="more"></a><h1 id="1、tf-idf" class="heading-control"><a href="#1、tf-idf" class="headerlink" title="1、tf-idf"></a>1、tf-idf<a class="heading-anchor" href="#1、tf-idf" aria-hidden="true"></a></h1><p>tf-idf是一种比较传统的文本表示方法，它首先为每个词计算出一个值，再组成向量来表示当前文档。它的大小等于词表数。首先tf是词频，也就是当前词在文档中出现的次数，通常会除以文档总词数来做归一化。idf的计算方法是log(语料库中总文档数 / 包含当前词的文档数)，可见分子是固定值，idf将随着包含当前词的文档数的增加而减小，也就是说常见词的idf值会相对较小，而当前文档比较有代表性的词发挥更大的作用。tf-idf的缺点是它是词袋模型，无法考虑词的位置信息，上下文信息以及一些分布特征。</p><h1 id="2、word2vec" class="heading-control"><a href="#2、word2vec" class="headerlink" title="2、word2vec"></a>2、word2vec<a class="heading-anchor" href="#2、word2vec" aria-hidden="true"></a></h1><p>实际上tf-idf就是one-hot的一种优化，还是存在维度灾难以及语义鸿沟的问题。因此后来的工作着重于构建<strong>分布式低维稠密词向量</strong>。word2vec就是它们的开山之作。我们知道NNLM（语言模型）是一种自监督训练的模型，用上文来预测下一个词的概率，那么词向量就可以作为它的副产物学习到这种基于序列共现的语境信息。word2vec基于这种思想提出了更专注于词向量学习的模型（比如舍弃隐藏层），用滑动窗口来指定固定大小的上下文，试图用当前词来预测上下文（skip-gram）或用上下文来预测当前词（CBOW）。具体细节可以参考<a href="https://arxiv.org/pdf/1411.2738.pdf" target="_blank" rel="noopener">这篇论文</a>。</p><p><strong>word2vec的两种加速训练策略</strong></p><ul><li><p>分层softmax<br>用哈夫曼树来计算词的概率，每个词对应一个叶节点。非叶节点也各自对应一个向量，词的概率可由它到根节点的唯一路径来计算。<br>哈夫曼树的构造方法：将词表中的每个词看作只有一个结点的树，用词频来表示它们的权重。选择根节点权重最小的两棵树合并，合并后的父节点权重等于两个子结点之和。下面是一个例子：</p><p><img src="https://img-blog.csdnimg.cn/20200225165605690.png#pic_center" alt><br>为了保证概率相加等于1，在路径上采用sigmoid来计算向左或向右（n表示结点，v表示结点向量）：</p></li></ul><script type="math/tex; mode=display">p(n, left) = \sigma (v^T\mathbf{h})</script><script type="math/tex; mode=display">p(n, right) = 1 - \sigma (v^T\mathbf{h}) = \sigma (-v^T\mathbf{h})</script><p>  那么某个词 $w_o$ 出现的概率就是：</p><p>  <img src="https://raw.githubusercontent.com/Anery/MyBlogPics/master/20200216224934.png" style="zoom: 50%;"><br>  [[·]]是个1/-1函数，左子结点取1，右子结点取-1。 再算cross entropy即可：</p><p><img src="https://raw.githubusercontent.com/Anery/MyBlogPics/master/20200216225025.png" style="zoom:50%;"><br>  这样就代替了softmax，复杂度从O(N)变成O(log N)。</p><ul><li><p>负采样</p><p>采样概率：在词频上取0.75次幂，减小词频差异带来的采样影响，即</p><script type="math/tex; mode=display">weight(w) = \frac{count(w)^{0.75}}{\sum_u count(u)^{0.75}}</script><p>那么损失函数为：</p><script type="math/tex; mode=display">E = -\log \sigma(v'^T_{w_o} \mathbf{h}) - \sum_{w_N \in NEG} \log \sigma(-v'^T_{w_N} \mathbf{h})</script><p><script type="math/tex">w_o</script> 是目标词，<script type="math/tex">w_I</script> 是输入词。对于skip-gram，<script type="math/tex">\mathbf{h} = v_{w_I}</script>，对于CBOW，<script type="math/tex">\mathbf{h} = \frac{1}{C}\sum_{c=1}^{C} v_{w_c}</script>。</p></li></ul><p>word2vec只能抽取局部特征，词的上下文信息局限于滑动窗口大小。</p><h1 id="3、GloVe" class="heading-control"><a href="#3、GloVe" class="headerlink" title="3、GloVe"></a>3、GloVe<a class="heading-anchor" href="#3、GloVe" aria-hidden="true"></a></h1><p>细节推荐<a href="https://blog.csdn.net/coderTC/article/details/73864097" target="_blank" rel="noopener">这篇博客</a>。主要的几个步骤包括：</p><ul><li><p>构建共现矩阵<br> GloVe指定特定大小的上下文窗口，通过滑动该窗口统计共现矩阵X（|V|*|V|），$X_{ij}$ 表示中心词i与上下文词j的共现次数。同时还定义了衰减函数，令距离为d的两个词在计数时乘以1/d。 </p></li><li><p>确定近似目标<br>作者发现可以用概率之比来建模共现关系。定义条件概率P:</p><script type="math/tex; mode=display">P_{ij} = P(j|i) = \frac{X_{ij}}{X_i}</script><p>表示词j出现在i上下文的概率。而用词k出现在i的上下文与它出现在j上下文的概率之比</p><script type="math/tex; mode=display">ratio_{i,j,k} = \frac{P_{ik}}{P_{jk}}</script><p>来表示i，j，k三个词之间的共现关系。当i，k和j，k相关程度近似时，该比率趋近于1；i，k相关度大于j，k相    关度时该比率值较大，反之则较小。GloVe的目标就是使学习到的词向量满足这样的规律，既有自身上下文信息，又能和其它词联系起来。目标函数：</p><script type="math/tex; mode=display">F(w_i,w_j,w_k) = \frac{P_{ik}}{P_{jk}}</script><p> 要同时满足三个词的约束关系，训练复杂度会变得很高。作者通过一系列变（硬）换（凑），把上式转换成了两个词的约束目标：</p><script type="math/tex; mode=display">\begin{aligned}  F(w_i,w_j,w_k) &= \exp((w_i - w_j)^Tw_k) \\                 &= \exp(w_i^Tw_k - w_j^Tw_k) \\                 &= \frac{\exp (w_i^Tw_k)}{\exp (w_j^Tw_k)} =          \frac{P_{ik}}{P_{jk}}  \end{aligned}</script><p>由此，</p><script type="math/tex; mode=display">P_{ik} = \exp (w_i^Tw_k)</script><p>成为新的目标函数。然而这种内积计算方式具有对称性，为了避免这种错误的性质，作者继续变（硬）换（凑）：</p><script type="math/tex; mode=display">P_{ik} = \frac{X_{ik}}{X_i} = \exp (w_i^Tw_k) \\  \log P_{ik} = \log (X_{ik}) - \log X_i = w_i^Tw_k \\  w_i^Tw_k + \log X_i = \log (X_{ik})</script><p>将$\log X_i$看作常数项，再添加一个偏置$b_k$，目标函数最终形式为：</p><script type="math/tex; mode=display">w_i^Tw_k + b_i + b_k = \log(X_{ik})</script><p>其中$X_{ik}$ 是共现矩阵中的值。</p></li><li><p>构造损失函数（平方损失）</p><script type="math/tex; mode=display">J = \sum_{i,k=1}^V f(X_{ik}) (w_i^Tw_k + b_i + b_k - \log(X_{ik}))^2</script><p>其中$f(X_{ij})$ 是关于共现矩阵的权重函数，</p><p>  <img src="https://raw.githubusercontent.com/Anery/MyBlogPics/master/20200216233258.png" style="zoom: 67%;"><br>也就是说，共现次数越少，对它们的相关性约束越小。</p></li></ul><p>推导过程见<a href="https://blog.csdn.net/coderTC/article/details/73864097" target="_blank" rel="noopener">这篇博客</a>或<a href="https://www.aclweb.org/anthology/D14-1162.pdf" target="_blank" rel="noopener">原论文</a>。</p><p><strong>Glove和Word2vec比较</strong></p><ul><li>word2vec面向局部特征，基于滑动窗口，而GloVe综合了全局语料。</li><li>word2vec可以增量学习，而Glove是由固定语料计算的共现矩阵。</li></ul><h1 id="4、Fasttext" class="heading-control"><a href="#4、Fasttext" class="headerlink" title="4、Fasttext"></a>4、Fasttext<a class="heading-anchor" href="#4、Fasttext" aria-hidden="true"></a></h1><p>Fasttext最早其实是一个文本分类算法，后续加了一些改进来训练词向量。概括了几点：</p><ul><li>fasttext在输入时对每个词加入了n-gram特征，在输出时使用分层softmax加速训练。</li><li>fasttext将整篇文章的词向量求平均作为输入得到文档向量，用文本分类做有监督训练，对输出进行softmax回归，词向量为副产品。</li><li>fasttext也可以无监督训练词向量，与CBOW非常相似。</li></ul><h1 id="5、ELMo" class="heading-control"><a href="#5、ELMo" class="headerlink" title="5、ELMo"></a>5、ELMo<a class="heading-anchor" href="#5、ELMo" aria-hidden="true"></a></h1><p>之前那些方法构造的都是独立于上下文的word embedding，也就是无论下游任务是什么，输入的embedding始终是固定的，这就无法解决一词多义，以及在不同语境下有不同表现的需求。所以后续的ELMo，GPT-2以及BERT都是针对于这类问题提出的，通过预训练和fine-tune两个阶段来构造context-dependent的词表示。</p><p>ELMo使用双向语言模型来进行预训练，用两个分开的双层LSTM作为encoder。biLM的loss是：</p><script type="math/tex; mode=display">L = \sum_{k=1}^N (\log p(t_k|t_1,...,t_{k-1};\overrightarrow{\Theta}_{LSTM},\Theta_s) + \log p(t_k|t_{k+1},...,t_{N};\overleftarrow{\Theta}_{LSTM},\Theta_s))</script><p>其中$\Theta_s$ 是softmax层参数。作者认为第一层学到的是句法信息，第二层学到的是语义信息。这两层LSTM的隐状态以及初始的输入加权求和就得到当前词的embedding。ELMo还设置了一个参数，不同的下游任务可以取特定的值，来控制ELMo词向量起到的作用。总体来说第k个token得到的预训练embedding就是：</p><script type="math/tex; mode=display">\mathbf{ELMo}_k^{task} = \gamma^{task} \sum_{j=0}^{L}s_j^{task} \mathbf{h}_{kj}^{LM}</script><p>在面对具体下游任务时，首先固定biLM的参数得到一个词表示，再与<strong>上下文无关的词表示</strong>（word2vec，或者charCNN获得的表示）拼接作为模型输入，在反向传播时fine-tune所有参数。</p><p>原文中提到的一些细节：</p><ul><li>biLM不同层的activation分布不同，在加权求和之前使用layer normalization有时会很有效。</li><li>增加dropout和L2正则化可能会有用，这对ELMo的权重提出了一个归纳偏差，使其接近所有biLM层的平均值。</li><li>在获得<strong>上下文无关词表示</strong>时，原文采用的方式是先用2048个charCNN卷积核做卷积，再过两层highway networks，然后用一个线性层把输出降到512维。</li></ul><h1 id="6、OpenAI-GPT" class="heading-control"><a href="#6、OpenAI-GPT" class="headerlink" title="6、OpenAI GPT"></a>6、OpenAI GPT<a class="heading-anchor" href="#6、OpenAI-GPT" aria-hidden="true"></a></h1><p>GPT和BERT与ELMo不同，ELMo使用LSTM作为编码器，而这两个用的是编码能力更强的Transformer。</p><p>GPT也是用语言模型进行大规模无监督预训练，但使用的是单向语言模型，也就是只根据上文来预测当前词。它实现的方式很直观，就是Transformer的decoder部分，只和前面的词计算self-attention来得到表示。在下游任务上，之前的ELMo相当于扩充了其它任务的embedding层，各个任务的上层结构各不相同，而GPT则不同，它要求所有下游任务都要完全与GPT的结构保持一致，只在输入输出形式上有所变化：</p><p><img src="https://raw.githubusercontent.com/Anery/MyBlogPics/master/20200218230527.png"></p><p>这是在NLP上第一次实现真正的端到端，不同的任务只需要定制不同的输入输出，无需构造内部结构。这样预训练学习到的语言学知识就能直接引入下游任务，相当于提供了先验知识。比如说人在做阅读理解时，先通读一遍全文再根据问题到文章中找回答，这些两阶段模型就类似这个过程。为了防止fine-tune时丢失预训练学到的语言知识，损失函数同时考虑下游任务loss（$L_2$）和语言模型loss（$L_1$）：</p><script type="math/tex; mode=display">L_3(C) = L_2(C) + \lambda L_1(C)</script><p>个人认为GPT的最大创新：</p><ul><li>用足够复杂的模型结构担任不同NLP任务的中间框架，启发了统一的端到端实现策略。</li><li>第二阶段保留语言模型的loss。</li></ul><h1 id="7、BERT" class="heading-control"><a href="#7、BERT" class="headerlink" title="7、BERT"></a>7、BERT<a class="heading-anchor" href="#7、BERT" aria-hidden="true"></a></h1><p>推荐<a href="https://jalammar.github.io/illustrated-bert/" target="_blank" rel="noopener">这篇博客</a>（这位大佬的其它文章质量也超高，尤其Transformer那篇估计是好多人的入门必看）</p><p>GPT虽然效果很好，但它在预训练时使用的是transformer的decoder部分，也就是单向语言模型，在计算attention时只能看见前面的内容，这样embedding获得的上下文信息就不完整。ELMo虽然是双向语言模型，但实际上是分开执行再组合loss，这就会带来一定的损失。</p><h2 id="7-1-Bert预训练" class="heading-control"><a href="#7-1-Bert预训练" class="headerlink" title="7.1 Bert预训练"></a>7.1 Bert预训练<a class="heading-anchor" href="#7-1-Bert预训练" aria-hidden="true"></a></h2><p>与GPT不同的是，bert在预训练时除了语言模型loss以外，还增加了一个“next sentence prediction”任务，即两个句子组成sentence pair同时输入，预测第二句是否是第一个句子的下文，是一个二分类任务。</p><h3 id="7-1-1-输入" class="heading-control"><a href="#7-1-1-输入" class="headerlink" title="7.1.1 输入"></a>7.1.1 输入<a class="heading-anchor" href="#7-1-1-输入" aria-hidden="true"></a></h3><ul><li>每个位置的输入：<ul><li><code>wordpiece-token</code> 词向量，这里的wordpiece是将token拆分成子词。</li><li><code>position emb</code> 位置向量</li><li><code>segment emb</code> 句子标识，属于第一个句子则为0，第二个句子则为1</li></ul></li><li>整体输入：<code>[CLS]</code> ; sent1 ; <code>[SEP]</code>; sent2 ;<code>[SEP]</code></li></ul><h3 id="7-1-2-训练任务" class="heading-control"><a href="#7-1-2-训练任务" class="headerlink" title="7.1.2 训练任务"></a>7.1.2 训练任务<a class="heading-anchor" href="#7-1-2-训练任务" aria-hidden="true"></a></h3><ul><li><p>Masked Language Model</p><ul><li><p>所谓双向LM，就是在预测当前词时同时考虑上文和下文，也就是</p><script type="math/tex; mode=display">p(t_k | t_1, ..., t_{k-1}, t_{k+1}, ...,t_N)</script><p>但LM是要逐词预测的，用这种概率计算方法会导致<strong>信息泄露</strong>，也就是当前词已经在之前的预测中作为下文而暴露了。作者由此提出了<strong>MASK</strong>策略，只选取15%的词进行预测，在输入时用[MASK]标记替代，而仍然以原词作为训练target。类似于阅读理解中的Cloze任务。当然，预测的词少了，模型收敛速度就会变慢，需要的训练step也要相应增加。</p></li><li><p>mask解决了信息泄露问题，但实际输入（也就是fine-tune阶段）不会包含这种标记，导致两阶段不一致，对训练效果产生影响。作者的解决方案是在这随机选取的15%词当中，80%的概率替换为[MASK]，10%的概率替换成其它词（负采样） ，10%的概率保留原词。这样有一个好处是，模型不知道当前要预测的词是否被篡改了，迫使其更关注上下文，学习到上下文相关的表示，这正是我们的目的。</p><p>作者还在附录里给出了一个扩展实验，对比不同的预训练mask策略对后续结果的影响：</p><p><img src="https://raw.githubusercontent.com/Anery/MyBlogPics/master/20200222142447.png" style="zoom: 80%;"><br>可以看到至少在这两个任务中，结果对不同的mask法是鲁棒的，差别不大。但从最后两条可以看出，直接去掉[MASK]，80%或100%取负样本的效果相比之下差了很多。按理说使用负样本相当于构建去噪自编码器，到底比MASK差在哪？思考了一下，原因很可能是负采样词作为其它词的上下文输入，使得这些词学到的embedding融合了错误的信息，对训练造成影响；而[MASK]本身并没有任何含义，它从未作为target出现过，也就没有特定的出现语境，因此其embedding没有实际意义，对其它词的影响也就相对较小。</p></li></ul></li><li><p>Next Sentence Prediction</p><p>0/1分类任务。从语料中选取两个片段AB（注意这里是两个“span”，而不是实际意义上的“句子”，因为希望输入尽可能长）作为一条输入，50%的概率AB连续（1），50%不连续（0）。输出在[CLS]处取FFNN+Softmax做二分类预测。输入的最大长度是512，超过则直接截断。</p></li></ul><h3 id="7-1-3-训练细节" class="heading-control"><a href="#7-1-3-训练细节" class="headerlink" title="7.1.3 训练细节"></a>7.1.3 训练细节<a class="heading-anchor" href="#7-1-3-训练细节" aria-hidden="true"></a></h3><ul><li><p>预训练数据及规模<br>BooksCorpus (800M words) 加 Wikipedia (2,500M words)</p></li><li><p>参数设置</p><ul><li>batch_size： 256 sequences(256*512 tokens) , step 1,000,000 (40 epochs on 3.3 billion word corpus)</li><li>Adam优化器。lr=1e-4，$\beta_1$=0.9，$\beta_2$=0.999，l2 weight decay=0.01</li><li>learning rate warmup：10,000 steps，lr线性缩减</li><li>所有层均设dropout=0.1</li><li>激活函数：gelu</li></ul></li><li><p>训练loss<br>masked LM与NSP的log likelihood之和</p></li></ul><h2 id="7-2-Bert-Fine-tune" class="heading-control"><a href="#7-2-Bert-Fine-tune" class="headerlink" title="7.2 Bert Fine-tune"></a>7.2 Bert Fine-tune<a class="heading-anchor" href="#7-2-Bert-Fine-tune" aria-hidden="true"></a></h2><p>fine-tuning的任务主要分成<strong>基于句子</strong>的和<strong>基于token</strong>的。基于句子的一般取[CLS]的embedding输出预测，基于token的则直接取对应位置的输出进行预测。</p><p>一般需根据特定的任务重新设置batch_size, learning rate, epochs超参数，其余与预训练保持一致即可。</p><h5 id="训练" class="heading-control"><a href="#训练" class="headerlink" title="训练"></a>训练<a class="heading-anchor" href="#训练" aria-hidden="true"></a></h5><p>fine-tuning的任务主要分成<strong>基于句子</strong>的和<strong>基于token</strong>的。基于句子的一般取[CLS]的embedding输出预测，基于token的则直接取对应位置的输出进行预测。</p><p>一般需根据特定的任务重新设置batch_size, learning rate, epochs超参数，其余与预训练保持一致即可。</p><p><img src="https://raw.githubusercontent.com/Anery/MyBlogPics/master/20200222154051.png" style="zoom: 67%;"></p><p>预训练好的Bert除了用于fine-tuning以外，还可以像ELMo一样作为特征抽取器，也就是直接用学习到的word embeddings当做其它模型的输入。目前看来最好的选择是最后四层向量拼接。</p><p>Bert与GPT的区别：</p><ul><li>GPT与Bert训练数据不同，GPT使用BooksCorpus (800M words); BERT是BooksCorpus (800M words)加Wikipedia (2,500M words)。</li><li>GPT在预训练时没有[CLS]和[SEP]，在下游任务时才有</li><li>GPT在fine-tuning时加入LM的loss，而Bert是完全使用任务特定的目标函数。</li><li>GPT的lr在两阶段保持一致，Bert认为任务特定的lr效果更好</li></ul><p>Bert最大的创新：</p><ul><li>用mask策略实现了双向语言模型，非常巧妙。</li><li>预训练除了语言模型，还加入了next sentence prediction，试图学习更高层面的语言关联性。提供了很好的扩展思路。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章记录词向量的发展历程，包括&lt;strong&gt;tf-idf、word2vec、GloVe、ELMo、OpenAI GPT&lt;/strong&gt;以及&lt;strong&gt;Bert&lt;/strong&gt;，只记录个人认为比较核心的内容，以及一些值得思考的边角细节。&lt;/p&gt;
    
    </summary>
    
      <category term="NLP" scheme="http://anery.github.io/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://anery.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>pytorch学习之nn.Embedding和nn.EmbeddingBag</title>
    <link href="http://anery.github.io/posts/556a9e21.html"/>
    <id>http://anery.github.io/posts/556a9e21.html</id>
    <published>2020-02-16T08:52:52.000Z</published>
    <updated>2020-02-16T09:03:27.537Z</updated>
    
    <content type="html"><![CDATA[<p>关于pytorch中的embedding常用的主要有两个函数，这篇blog将从每个参数的含义入手，通过举例的方式比较两个方法的异同以及各自的使用场景。</p><a id="more"></a><p>从基础的nn.Embedding说起：</p><blockquote><p>CLASS torch.nn.Embedding(<strong>num_embeddings, embedding_dim,<br>padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False, _weight=None</strong>)</p></blockquote><p>num_embeddings, embedding_dim没啥好说的，就是look-up表的形状，我们在搭建网络时大多情况下只用得上这两个参数。下面具体看看剩下的参数能做什么：</p><ul><li>padding_idx<br>表示pad的序号。NLP项目中句子需pad成相同长度批量输入，此项即为填充项对应的index，对应的embedding为0.<br><img src="https://img-blog.csdnimg.cn/20191117233502400.png" alt></li><li>max_norm<br>用来约束embedding vector，把范数大于max_norm的vector重归一化，使之等于max_norm。</li><li>norm_type<br>p-norm的p值（0,1,2）<br>这两个参数基本不用了，现在都用kaiming和xavier初始化参数</li><li>scale_grad_by_freq<br>顾名思义，用词的频率来缩放梯度，即梯度除以这个词的出现次数。注意这里的词频指的是自动获取当前mini-batch中的词频，而非对于整个词典。</li><li>sparse<br>bool值，设置成True时参数weight为稀疏tensor。<br>所谓稀疏tensor是说反向传播时只更新当前使用词的embedding，加快更新速度。这里值得一提的是，即使设置sparse=True，embedding的权重也未必稀疏更新：（1）与优化器相关，使用momentumSGD、Adam等优化器时包含momentum项，导致不相关词的embedding依然会叠加动量，无法稀疏更新；（2）使用weight_decay，即正则项计入loss。</li></ul><p>好了，明白了Embedding的参数，再来看EmbeddingBag：</p><blockquote><p>CLASS torch.nn.EmbeddingBag(<strong>num_embeddings, embedding_dim, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, mode=’mean’, sparse=False, _weight=None</strong>)</p></blockquote><p>官方API：</p><p> <a href="https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag" target="_blank" rel="noopener">https://pytorch.org/docs/stable/nn.htmlhighlight=embeddingbag#torch.nn.EmbeddingBag</a></p><p>参数只多了一个：mode，先来看这个参数的含义。</p><p>官网上说得很清楚，取值分三种，对应三种操作：”sum”表示普通embedding后接torch.sum(dim=0)，”mean”相当于后接torch.mean(dim=0)，”max”相当于后接torch.max(dim=0)。</p><p>只看这个参数就清楚了，EmbeddingBag就是把look-up表整合成一个embedding，当不需要具体查表获得embedding，只需要一个整合结果时，它比上述两阶段操作更高效。</p><p>来看它的输入：</p><ul><li><p>input (LongTensor)和offsets (LongTensor, optional)<br>input可以是2D或1D：</p><ul><li>input shape 2D (B,N)<br>相当于B个bag，每个bag长度固定为N，此时要求offsets参数为None。<br>输出分别对B个bag做整合，shape：(B, embedding_dim)</li><li>input shape 1D (N)<br>虽然是1D，但默认为多个bag平铺在了一起，因此offsets必须同时输入，表示每个bag的起始index，shape=(B)，再分别对每个bag整合。<br>输出shape：(B, embedding_size)</li></ul><p>说到这可以发现其实和类名一样，这就是个“词袋”操作，典型的应用场景是FastText，多个文档平铺成1D输入，再指定offsets，直接就可以进行批量不等长文档处理，写起来简单，效率又有提升。</p></li></ul><p>官方的例子：<br>    <img src="https://img-blog.csdnimg.cn/20191118222612392.png" alt></p><ul><li>per_sample_weights(Tensor, optional)<br>  该输入给每个实例一个权重再加权求和（此时mode只能为sum），与输入shape相同。<br>一个典型的应用场景是deepFM，某列特征对应的embedding有时需按照权重加和。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;关于pytorch中的embedding常用的主要有两个函数，这篇blog将从每个参数的含义入手，通过举例的方式比较两个方法的异同以及各自的使用场景。&lt;/p&gt;
    
    </summary>
    
      <category term="NLP" scheme="http://anery.github.io/categories/NLP/"/>
    
    
      <category term="pytorch" scheme="http://anery.github.io/tags/pytorch/"/>
    
      <category term="NLP" scheme="http://anery.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>pytorch中LSTM的细节分析理解</title>
    <link href="http://anery.github.io/posts/f14fa36f.html"/>
    <id>http://anery.github.io/posts/f14fa36f.html</id>
    <published>2020-02-16T08:46:35.000Z</published>
    <updated>2020-02-16T09:11:56.148Z</updated>
    
    <content type="html"><![CDATA[<p>虽然看了一些很好的blog了解了LSTM的内部机制，但对框架中的lstm输入输出和各个参数还是没有一个清晰的认识，今天打算彻底把理论和实现联系起来，再分析一下pytorch中的LSTM实现。</p><a id="more"></a><p>先说理论部分。<a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">一个非常有名的blog</a>把原理讲得很清楚，推荐参考。总之就是这些公式：</p><p><img src="https://img-blog.csdnimg.cn/20190820143026202.png" alt></p><p>简单来说就是，LSTM一共有三个门，输入门，遗忘门，输出门，$i,f,o$分别为三个门的程度参数，$g$是对输入的常规RNN操作。公式里可以看到LSTM的输出有两个，细胞状态$c’$和隐状态$h’$，$c’$是经输入、遗忘门的产物，也就是当前cell本身的内容，经过输出门得到$h’$，就是想输出什么内容给下一单元。</p><p>那么实际应用时，我们并不关心细胞本身的状态，而是要拿到它呈现出的状态$h’$作为最终输出。以pytorch中的LSTM为例：</p><p><strong>torch.nn.LSTM(*args, **kwargs)</strong></p><p>官方API：<br><a href="https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.LSTM" target="_blank" rel="noopener">https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.LSTM</a></p><hr><ul><li>参数<br>— <strong>input_size</strong><br>— <strong>hidden_size</strong><br>— <strong>num_layers</strong><br>— <strong>bias</strong><br>— <strong>batch_first</strong><br>— <strong>dropout</strong><br>— <strong>bidirectional</strong></li></ul><hr><ul><li>输入<br>— <strong>input</strong>  (seq_len, batch, input_size)<br>— <strong>h_0</strong> (num_layers <em> num_directions, batch, hidden_size)<br>— <strong>c_0</strong> (num_layers </em> num_directions, batch, hidden_size)</li></ul><hr><ul><li>输出<br>— <strong>output</strong> (seq_len, batch, num_directions <em> hidden_size)<br>— <strong>h_n</strong> (num_layers </em> num_directions, batch, hidden_size)<br>— <strong>c_n</strong> (num_layers * num_directions, batch, hidden_size)</li></ul><hr><p>用起来很简单，当作黑箱时只要设置参数让它输出我们想要的shape就行了，但这些参数好像很难和前面公式里的那些联系起来，不便于理解和灵活使用。</p><p>先看一张很好的图（<a href="https://www.zhihu.com/question/41949741/answer/318771336" target="_blank" rel="noopener">LSTM神经网络输入输出究竟是怎样的？ - Scofield的回答 - 知乎</a>）：</p><p><img src="https://img-blog.csdnimg.cn/20190820153259157.png" alt></p><p>这张图是以MLP的形式展示LSTM的传播方式（不用管左边的符号，输出和隐状态其实是一样的），方便理解hidden_size这个参数。其实hidden_size在各个函数里含义都差不多，就是参数W的第一维（或最后一维）。那么对应前面的公式，hidden_size实际就是以这个size设置所有W的对应维。</p><p>再看另一张很好的图（<a href="https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714%29" target="_blank" rel="noopener">https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714</a>）：</p><p><img src="https://img-blog.csdnimg.cn/20190820152709292.png" alt></p><p>这张图非常便于理解参数num_layers。实际上就是个depth堆叠，每个蓝色块都是LSTM单元，只不过第一层输入是<script type="math/tex">x_t,h_{t-1}^{(0)},c_{t-1}^{(0)}</script>，中间层输入是<script type="math/tex">h_{t}^{(k-1)},h_{t-1}^{(k)},c_{t-1}^{(k)}</script>。</p><p>剩下的参数就比较好理解了，input_size即输入的隐层维度，比如embedding_dim。batch_first，第一维是否是batch，为什么要设置这个参数后面再说。bidirectional，是否为双向LSTM。</p><p>接下来看一下输入输出。关于input，API中提到也可以是一个packed变量序列，这个后面再讲。输入输出中$h$和$c$的shape (num_layers * num_directions, batch, hidden_size) 也是个容易困惑的点，但有了上面那张图就好说多了。绿色块$h_n,c_n$即长度为n的序列的最终输出，可以看出是所有depth输出的拼接，维度是num_layers。双向LSTM情况，相当于有两个图中的网络，只不过输入颠倒过来了，再将这两个最终隐状态拼接起来，维度num_layers*2。</p><p>最后看一下输出output。初学时看别人的代码，总是搞不清到底是取output还是用$h_n$，怎么用的都有。其实从图中可以看到，output就是最后一个layer上，序列中每个状态$h$的集合（若为双向则按位置拼接，输出维度2*hidden_size），所以$h_n$就是$output[-1,:,:]$。我们使用LSTM的目的是得到整个序列的embedding，与序列长度无关，由于LSTM具有序列信息传递性，因此一般都取$h_n$当作序列输出。但双向LSTM推广后，每个位置的隐层输出都可以作为当前词的一个融合了上下文的embedding，因此取output再以词粒度进行融合（比如attention加权求和、pooling）也是一种主流方法。</p><p>理论终于和实践联系起来了，下面来具体分析一下pytorch的LSTM实现。</p><h2 id="pytorch的LSTM" class="heading-control"><a href="#pytorch的LSTM" class="headerlink" title="pytorch的LSTM"></a>pytorch的LSTM<a class="heading-anchor" href="#pytorch的LSTM" aria-hidden="true"></a></h2><hr><p><strong>1、torch.nn.LSTMCell(input_size, hidden_size, bias=True)</strong></p><p>官方API：</p><p><a href="https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.LSTMCell" target="_blank" rel="noopener">https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.LSTMCell</a>**</p><p>一个LSTM单元。相当于一个time step的处理，应该是对应TensorFlow里类似的实现。基本不用。</p><p><strong>2、torch.nn.LSTM(*args, **kwargs)</strong></p><p>官方API：</p><p><a href="https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.LSTM" target="_blank" rel="noopener">https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.LSTM</a></p><p>前面基本讲得差不多了，只剩下两处：参数batch_first和input的packed variable length sequence。</p><p>为什么要有batch_first这个参数呢？常规的输入不就是(batch, seq_len, hidden_size)吗？而且参数默认为False，也就是它鼓励你第一维不是batch，更奇怪了。</p><p>取pytorch官方的一个tutorial（<a href="https://pytorch.org/tutorials/beginner/chatbot_tutorial.html" target="_blank" rel="noopener">chatbot tutorial</a>）中的一个图：</p><p><img src="https://img-blog.csdnimg.cn/20190820164135998.png" alt></p><p>左边是我们的常规输入（先不考虑hidden dim，每个数字代表序列中的一个词），右边是转置后，第一维成了max_length。我们知道在操作时第一维一般可视为“循环”维度，因此左边一个循环项是一个序列，无法同时经LSTM处理，而右边跨batch的循环项相当于当前time step下所有序列的当前词，可以并行过LSTM。（当然不管你是否batch_first它都是这么处理的，这个参数应该只是提醒一下这个trick）</p><h4 id="pack-amp-pad" class="heading-control"><a href="#pack-amp-pad" class="headerlink" title="pack&amp;pad"></a>pack&amp;pad<a class="heading-anchor" href="#pack-amp-pad" aria-hidden="true"></a></h4><p>（感觉说起来没那么简单，所以加了个小标题。）</p><p>前面说过nn.LSTM的输入也可以是“packed”形式，那么这是个什么形式？</p><p>先不问为什么，看一下pack和pad的操作是怎样的。</p><hr><p><strong>torch.nn.utils.rnn.pack_sequence(sequences, enforce_sorted=True)</strong></p><p>官方API：<a href="https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.utils.rnn.pack_sequence" target="_blank" rel="noopener">https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.utils.rnn.pack_sequence</a></p><p>这是pack操作，输入的sequences是tensor组成的list，要求按长度从大到小排序。官网的例子：</p><p><img src="https://img-blog.csdnimg.cn/20190820171602978.png" alt></p><hr><p><strong>torch.nn.utils.rnn.pad_sequence(sequences, batch_first=False, padding_value=0)</strong></p><p>官方API：</p><p><a href="https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.utils.rnn.pad_sequence" target="_blank" rel="noopener">https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.utils.rnn.pad_sequence</a></p><p>这是pad操作，sequences也是list。这个比较好理解，就是给list里的tensor都用padding_value来pad成最长的长度，并组合成一个tensor：</p><p><img src="https://img-blog.csdnimg.cn/20190820172338569.png" alt></p><p>看了这两个操作，隐隐约约和前面的LSTM联系起来了。我们知道一个batch里的序列不一定等长，需要pad操作用0把它们都填充成max_length长度。但前面说了LSTM的一次forward对应一个time step，接收的是across batches的输入，这就导致短序列可能在当前time step上已经结束，而你还是在给它输入东西（pad），这就会对结果产生影响（可以对照公式看看，即便输入全0还是会有影响）。我们想要的效果是，LSTM知道batch中每个序列的长度，等到某个序列输入结束后下面的time step就不带它了。</p><p>传统的pad不能用，LSTM需要一种其它的方法来处理变长输入。这时我们观察刚看到的pack操作，感觉终于明白了它的道理。官方的例子有点混淆，我写了一个更直观的：</p><p><img src="https://img-blog.csdnimg.cn/20190820181026673.png" alt></p><p>把这个例子看成是LSTM处理一个batch的过程，注意看成转置的形式，即batch_first=False，也就是[4,1,9]是第一个序列，[5,2]是第二个序列…max_length=3，batch_size=5。从输出可以看出其实是一个很简单的过程，有点像稀疏矩阵的存储方法，先都塞到一起再记录位置（这里是长度）。</p><p>这两个函数都是基本操作，一般不会直接使用。常用的是下面这两个：</p><hr><p><strong>torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False, enforce_sorted=True)</strong></p><p>官方API：</p><p><a href="https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.utils.rnn.pack_padded_sequence" target="_blank" rel="noopener">https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.utils.rnn.pack_padded_sequence</a></p><p>顾名思义，pack一个经过pad的sequence，因为我们一般在处理数据时就已经将序列pad成等长了。lengths即为序列的长度。</p><hr><p><strong>torch.nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False, padding_value=0.0, total_length=None)</strong></p><p>官方API：</p><p><a href="https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.utils.rnn.pad_packed_sequence" target="_blank" rel="noopener">https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.utils.rnn.pad_packed_sequence</a></p><p>这是上面函数的逆操作，再pad回去供后续使用。这里的total_length是个很实用的参数，在下面的例子中可以看到。</p><hr><p>一个完整的例子：</p><p><img src="https://img-blog.csdnimg.cn/20190820203610836.png" alt><br><img src="https://img-blog.csdnimg.cn/20190820203739954.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20190820204233403.png" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">from</span> torch.nn.utils.rnn <span class="keyword">import</span> pack_padded_sequence, pad_packed_sequence</span><br><span class="line"></span><br><span class="line">a = t.tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">6</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">0</span>]]) <span class="comment">#(batch_size, max_length)</span></span><br><span class="line">lengths = t.tensor([<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 排序</span></span><br><span class="line">a_lengths, idx = lengths.sort(<span class="number">0</span>, descending=<span class="literal">True</span>)</span><br><span class="line">_, un_idx = t.sort(idx, dim=<span class="number">0</span>)</span><br><span class="line">a = a[idx]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义层 </span></span><br><span class="line">emb = t.nn.Embedding(<span class="number">20</span>,<span class="number">2</span>,padding_idx=<span class="number">0</span>) </span><br><span class="line">lstm = t.nn.LSTM(input_size=<span class="number">2</span>, hidden_size=<span class="number">4</span>, batch_first=<span class="literal">True</span>) </span><br><span class="line"></span><br><span class="line">a_input = emb(a)</span><br><span class="line">a_packed_input = t.nn.utils.rnn.pack_padded_sequence(input=a_input, lengths=a_lengths, batch_first=<span class="literal">True</span>)</span><br><span class="line">packed_out, _ = lstm(a_packed_input)</span><br><span class="line">out, _ = pad_packed_sequence(packed_out)</span><br><span class="line"><span class="comment"># 根据un_idx将输出转回原输入顺序</span></span><br><span class="line">out = t.index_select(out, <span class="number">0</span>, un_idx)</span><br></pre></td></tr></table></figure><p>上面便是常用的使用方法（个人认为完全可以封装到LSTM函数里，不知道为什么要这么设计）。但此时假设另一个batch，b：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># b是另一个batch</span></span><br><span class="line">b = t.tensor([[<span class="number">7</span>,<span class="number">8</span>,<span class="number">0</span>],[<span class="number">9</span>,<span class="number">0</span>,<span class="number">0</span>],[<span class="number">10</span>,<span class="number">0</span>,<span class="number">0</span>]])</span><br></pre></td></tr></table></figure><p>batch中的最大长度为2，而对于整个数据流来说max_length=3，这就导致b经LSTM后pad的结果与整体的长度不匹配，此时设置pack_padded_sequence的total_length=3即可。</p><hr><p>有不正确的地方还请指正！</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;虽然看了一些很好的blog了解了LSTM的内部机制，但对框架中的lstm输入输出和各个参数还是没有一个清晰的认识，今天打算彻底把理论和实现联系起来，再分析一下pytorch中的LSTM实现。&lt;/p&gt;
    
    </summary>
    
      <category term="NLP" scheme="http://anery.github.io/categories/NLP/"/>
    
    
      <category term="pytorch" scheme="http://anery.github.io/tags/pytorch/"/>
    
      <category term="NLP" scheme="http://anery.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>conda 导出环境/导入环境/导出base环境</title>
    <link href="http://anery.github.io/posts/dce5d55.html"/>
    <id>http://anery.github.io/posts/dce5d55.html</id>
    <published>2020-02-16T08:46:10.000Z</published>
    <updated>2020-02-16T08:57:22.052Z</updated>
    
    <content type="html"><![CDATA[<p>conda的虚拟环境真的非常实用，尤其是对于大的深度学习项目，给每个项目单独配一个环境，轻巧又容易管理，还能直接用别人配好的虚拟环境，非常方便。这里记录几个常用的导入导出命令免得每次找。</p><a id="more"></a><p>查看可用环境：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda info --envs</span><br></pre></td></tr></table></figure></p><p>输出样式：</p><p><img src="https://img-blog.csdnimg.cn/20190614200754939.png" alt></p><p>更换环境（如py36）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source activate py36</span><br></pre></td></tr></table></figure><p>导出当前环境：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda env export &gt; py36.yaml</span><br></pre></td></tr></table></figure><p>会生成一个<code>py36.yaml</code>文件，将其复制到目标机上后执行导入环境操作：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda env create -f py36.yaml</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：若导出base环境，则在目标机上会提示已存在（而且base环境无法删除）。所以要想导出base，最好先复制一下，再导出复制品：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n new_name --clone base</span><br></pre></td></tr></table></figure><p>再导出new_name环境即可。必要的话再在原机删除复制环境：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda remove -n new_name --all</span><br></pre></td></tr></table></figure></p><p>在用的时候发现有些module还是未安装，上网找了下原因，原来以上只会导出conda命令直接安装的包，而我的包大多是用pip安装在Anaconda的lib和site-package里了。因此还要用导出pip的方法：</p><p>pip导出安装的库到<code>27.txt</code>：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip freeze &gt; 27.txt</span><br></pre></td></tr></table></figure></p><p>pip导入<code>27.txt</code>中列出的库到新机：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r 27.txt</span><br></pre></td></tr></table></figure><p>其实就是按列表重新安装一遍，导出的列表可以自己先看一眼，筛掉一些脑抽安装的没用包</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;conda的虚拟环境真的非常实用，尤其是对于大的深度学习项目，给每个项目单独配一个环境，轻巧又容易管理，还能直接用别人配好的虚拟环境，非常方便。这里记录几个常用的导入导出命令免得每次找。&lt;/p&gt;
    
    </summary>
    
      <category term="tools" scheme="http://anery.github.io/categories/tools/"/>
    
    
      <category term="用法记录" scheme="http://anery.github.io/tags/%E7%94%A8%E6%B3%95%E8%AE%B0%E5%BD%95/"/>
    
  </entry>
  
  <entry>
    <title>transE(Translating Embedding)详解+简单python实现</title>
    <link href="http://anery.github.io/posts/e68d7ca3.html"/>
    <id>http://anery.github.io/posts/e68d7ca3.html</id>
    <published>2020-02-16T08:40:34.000Z</published>
    <updated>2020-02-16T08:57:22.062Z</updated>
    
    <content type="html"><![CDATA[<p><strong>表示学习</strong>旨在学习一系列低维稠密向量来表征语义信息，而<strong>知识表示学习</strong>是面向知识库中实体和关系的表示学习。当今大规模知识库（或称知识图谱）的构建为许多NLP任务提供了底层支持，但由于其规模庞大且不完备，如何高效存储和补全知识库成为了一项非常重要的任务，这就依托于知识表示学习。</p><p>transE算法就是一个非常经典的知识表示学习，用分布式表示（distributed representation）来描述知识库中的三元组。想象一下，这类表示法既避免了庞大的树结构构造，又能通过简单的数学计算获取语义信息，因此成为了当前表示学习的根基。</p><a id="more"></a><h1 id="1-transE算法原理" class="heading-control"><a href="#1-transE算法原理" class="headerlink" title="1 transE算法原理"></a>1 transE算法原理<a class="heading-anchor" href="#1-transE算法原理" aria-hidden="true"></a></h1><p>我们知道知识图谱中的事实是用三元组 $(h,l,t)$ 表示的，那么如何用低维稠密向量来表示它们，才能得到这种依赖关系呢？transE算法的思想非常简单，它受word2vec平移不变性的启发，希望$h+l≈t$（此为归纳偏差？）。</p><p>光有这一个约束可不够。想让$h+l≈t$，如何设置损失函数是个关键。我们发现表示学习都没有明显的监督信号，也就是不会明确告诉模型你学到的表示正不正确，那么想要快速收敛就得引入“相对”概念，即<strong>相对负例来说，正例的打分要更高</strong>，方法学名“<strong>negative sampling</strong>”。损失函数设计如下：</p><p><img src="https://img-blog.csdnimg.cn/20190504222759459.png" alt></p><p>其中$(h’,l,t’)$称为corrupted triplet，是随机替换头或尾实体得到（非同时，其实也可以替换relation）。$\gamma$为margin。细看发现这就是SVM的soft margin损失函数，所以可以说，transE针对给定三元组进行二分类任务，其中负例是通过替换自行构造的，目标是使得最相近的正负例样本距离最大化。</p><p>论文中给出了详细的算法流程：</p><p><img src="https://img-blog.csdnimg.cn/20190504224500785.png" alt></p><p>其中距离度量方式有L1范数和L2范数两种。在测试时，以一个三元组为例，用语料中所有实体替换当前三元组的头实体计算距离$d(h’+l,t)$，将结果按升序排序，用正确三元组的排名情况来评估学习效果（同理对尾实体这样做）。度量标准选择hits@10和mean rank，前者代表命中前10的次数/总查询次数，后者代表正确结果排名之和/总查询次数。</p><p>还有一点值得一提，文中给了两种测试结果raw和filter，其动机是我们在测试时通过替换得到的三元组并不一定就是负例，可能恰巧替换对了（比如（奥巴马，总统，美国）被替换成了（特朗普，总统，美国）），那么它排名高也是正确的，把当前三元组挤下去也正常。<strong>（存疑：这样的话训练时是否也应当过滤corrupted triplet呢）</strong> 所以测试时在替换后要检查一下新三元组是否出现在训练集中，是的话就删掉，这就是filter训练方法（不检查的是raw）。</p><h1 id="2-transE算法的简单python实现" class="heading-control"><a href="#2-transE算法的简单python实现" class="headerlink" title="2 transE算法的简单python实现"></a>2 transE算法的简单python实现<a class="heading-anchor" href="#2-transE算法的简单python实现" aria-hidden="true"></a></h1><p>为了更好地理解（其实是因为看不懂别人的），用python简单实现了transE算法，使用数据集FB15k。这里记录一些细节和几个小坑。完整代码见<a href="https://github.com/Anery/transE" target="_blank" rel="noopener">github</a>。</p><h4 id="1-训练transE" class="heading-control"><a href="#1-训练transE" class="headerlink" title="1. 训练transE"></a>1. 训练transE<a class="heading-anchor" href="#1-训练transE" aria-hidden="true"></a></h4><ul><li>Tbatch更新：在update_embeddings函数中有一个deepcopy操作，目的就是为了批量更新。这是ML中mini-batch SGD的一个通用的训练知识，在实际编码时很容易忽略。</li><li>两次更新：update_embeddings函数中，要对correct triplet和corrupted triplet都进行更新。虽然写作$(h,l,t)$和$(h’,l,t’)$，但两个三元组只有一个entity不同（前面说了，不同时替换头尾实体），所以在每步更新时重叠的实体要更新两次（和更新relation一样），否则就会导致后一次更新覆盖前一次。</li><li>关于L1范数的求导方法：先对L2范数求导，逐元素判断正负，为正赋值为1，负则为-1。</li><li>超参选择：对FB15k数据集，epoch选了1000（其实不需要这么大，后面就没什么提高了），nbatches选了400（训练最快），embedding_dim=50, learning_rate=0.01, margin=1。</li></ul><h4 id="2-测试" class="heading-control"><a href="#2-测试" class="headerlink" title="2. 测试"></a>2. 测试<a class="heading-anchor" href="#2-测试" aria-hidden="true"></a></h4><ul><li>isFit参数：区分raw和filter。filter会非常慢。</li></ul><h1 id="3-transE算法的局限性" class="heading-control"><a href="#3-transE算法的局限性" class="headerlink" title="3  transE算法的局限性"></a>3  transE算法的局限性<a class="heading-anchor" href="#3-transE算法的局限性" aria-hidden="true"></a></h1><p>transE效果很好且非常简单，后续大量的工作都是在此基础上的改进（简称trans大礼包），传统方法已经基本不用了（有些思想还是值得借鉴的，比如矩阵分解、双线性模型）。改进大体针对以下几个问题：</p><ul><li><strong>复杂关系建模效果差</strong>。对1-N,N-1,N-N关系，会出现冲突映射，一个实体在不同三元组内的表示融合，导致不明确甚至错误的语义信息。</li><li><strong>多源信息融合。</strong> 如何充分利用知识库中的额外信息（如实体类型、实体描述）。</li><li><strong>关系路径建模</strong>。 对relation之间的依赖进行建模。</li></ul><p>理解或实现有错误欢迎指出！</p><p>参考文献：</p><p>[1] Bordes A, Usunier N, Garcia-Duran A, et al. Translating embeddings for modeling multi-relational data[C]//Advances in neural information processing systems. 2013: 2787-2795.</p><p>[2] 刘知远, 孙茂松, 林衍凯, et al. 知识表示学习研究进展[J]. 计算机研究与发展, 2016, 53(2):247-261.</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;表示学习&lt;/strong&gt;旨在学习一系列低维稠密向量来表征语义信息，而&lt;strong&gt;知识表示学习&lt;/strong&gt;是面向知识库中实体和关系的表示学习。当今大规模知识库（或称知识图谱）的构建为许多NLP任务提供了底层支持，但由于其规模庞大且不完备，如何高效存储和补全知识库成为了一项非常重要的任务，这就依托于知识表示学习。&lt;/p&gt;
&lt;p&gt;transE算法就是一个非常经典的知识表示学习，用分布式表示（distributed representation）来描述知识库中的三元组。想象一下，这类表示法既避免了庞大的树结构构造，又能通过简单的数学计算获取语义信息，因此成为了当前表示学习的根基。&lt;/p&gt;
    
    </summary>
    
      <category term="NLP" scheme="http://anery.github.io/categories/NLP/"/>
    
    
      <category term="NLP" scheme="http://anery.github.io/tags/NLP/"/>
    
      <category term="Graph" scheme="http://anery.github.io/tags/Graph/"/>
    
      <category term="知识图谱" scheme="http://anery.github.io/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
  </entry>
  
  <entry>
    <title>主成分分析(PCA)的推导与理解</title>
    <link href="http://anery.github.io/posts/b160a645.html"/>
    <id>http://anery.github.io/posts/b160a645.html</id>
    <published>2020-02-14T13:08:59.000Z</published>
    <updated>2020-02-16T09:13:41.837Z</updated>
    
    <content type="html"><![CDATA[<h3 id="一、PCA简介" class="heading-control"><a href="#一、PCA简介" class="headerlink" title="一、PCA简介"></a>一、PCA简介<a class="heading-anchor" href="#一、PCA简介" aria-hidden="true"></a></h3><p>主成分分析（PCA）是一种常见的，也是最简单的降维手段，在机器学习中可用于特征提取。即便有时收集到的样本维数很高（即含有过多特征），但与学习任务相关的可能只是某个低维分布，这时就需要有效降维，在缓解维数灾难的同时令得到的低维嵌入仍能很好地描述原样本空间。</p><a id="more"></a><h3 id="二、PCA推导" class="heading-control"><a href="#二、PCA推导" class="headerlink" title="二、PCA推导"></a>二、PCA推导<a class="heading-anchor" href="#二、PCA推导" aria-hidden="true"></a></h3><p>首先进行任务描述。</p><p>设样本 $X∈R^{n \times d}$ , 低维表示$Z∈R^{n \times l} (l&lt;d)$ 。$Z=X \times W$。可以将这个变换看作坐标变换，那么$W$就是$l$个正交基向量（列向量）组成的矩阵，$W^TW=I$。也就是说，我们有n个d维的原样本x，经坐标变换后得到n个$l$维的新样本z。在新坐标系中，$Z$的每一行（即每个新样本）都可看作是原样本在$w_i$方向上的<strong>投影</strong>。为了更直观地进行坐标变换，我们将样本进行中心化，每个样本都减去均值，使得样本中心落在原点，$\sum_i{x_i=0}$。</p><p>我们知道投影可以用内积的形式表示，用$x_iw_1$表示$x_i$（行向量）在$w_1$上的投影长度（还要除以$w_1$的模，这里为基向量，模长为1）。在信号处理中，认为信号的方差较大，噪声方差较小，这个理论可以推广到我们的问题中，也就是希望投影后的样本点比较分散（方差大），如下图（网图侵删）：</p><p><img src="https://img-blog.csdnimg.cn/20181203105105902.png" alt></p><p>认为第一个投影方向更好，它使得样本投影方差最大。由于样本已中心化，因此方差可用投影长度的均方来表示：<script type="math/tex">\frac{1}{n}\sum_i^n{(x_iw_1)^2}</script><br>上式取极大就是PCA目标函数的一种形式，利用的是最大方差准则（也可以用最小二乘等准则）。其中$w_1$是要求的第一个投影方向（主方向，对应着W的某一列）。$x_iw_1$是一个标量，它的转置等于本身，目标函数可写成如下形式：</p><script type="math/tex; mode=display">\frac{1}{n}\sum_i^n{(x_iw_1)^T(x_iw_1)} \\=\frac{1}{n}\sum_i^n{w_1^Tx_i^Tx_iw_1} \\=\frac{1}{n}w_1^T(\sum_i^n{x_i^Tx_i} )w_1</script><p><strong>（注：这种方式是机器学习中常用的一种变换，将向量乘积根据需要灵活地结合成矩阵或标量，引入一些可解释性）</strong></p><p>显然中间部分就是协方差矩阵（$x_i$是行向量）。为了更形式化地表示，直接用$X$代替，并加上之前说的正交基假设，写出完整的目标函数：</p><script type="math/tex; mode=display">max \quad \frac{1}{n}w_1^T(XX^T)w_1 \qquads.t. \quad w_1^Tw_1=1</script><p>是一个约束最优化问题，可引入拉格朗日乘子求解。这里补充一句，我们的目标函数包含一个标准二次型，之所以有最优解是因为它是半正定的（协方差矩阵是半正定矩阵）。目标函数中的$\frac{1}{n}$对结果无影响，在计算时不考虑。引入拉格朗日乘子λ：</p><script type="math/tex; mode=display">L=w_1^T(XX^T)w_1-\lambda(w_1^Tw_1-1)</script><p>对参数求偏导得：</p><script type="math/tex; mode=display">\frac{\partial L}{\partial{w_1}}=0</script><script type="math/tex; mode=display">(XX^T)w_1=\lambda w_1</script><p>可以看出，为了求目标函数极大，拉格朗日乘子$\lambda$就是协方差矩阵的特征值，我们要求的投影方向向量$w_1$是该特征值对应的特征向量。上面等式两边同时乘$w_1^T$得：</p><script type="math/tex; mode=display">w_1^T(X^TX)w_1=\lambda</script><p>得到的式子左边就是我们的目标函数，那么目标函数可转换为</p><script type="math/tex; mode=display">max \quad \lambda</script><p>至此，我们将求最大投影方差问题转换成了求协方差矩阵最大特征值的问题。上面是用一个投影向量$w_1$为例，为了求得d个投影向量组成的变换矩阵W，只需求最大d个特征值对应的特征向量即可。</p><p>得到了$W$以后，我们就可以对样本进行降维处理得到$Z:Z=XW$。</p><h3 id="三、总结" class="heading-control"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结<a class="heading-anchor" href="#三、总结" aria-hidden="true"></a></h3><p>上面经过严格的目标函数推导，我们得出了主成分分析的求解方法。可以看出这个结果非常“巧合”地映射到了协方差矩阵的特征向量上，使得这个过程有了可解释性。</p><p>因为$l&lt;d$,很显然这是一个有损变换。从任务角度来说，我们在特征提取时舍弃了$d-l$个特征，造成了信息丢失。为了减少损失，在降维过程中应尽量舍弃那些用处不大的特征。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;一、PCA简介&quot;&gt;&lt;a href=&quot;#一、PCA简介&quot; class=&quot;headerlink&quot; title=&quot;一、PCA简介&quot;&gt;&lt;/a&gt;一、PCA简介&lt;/h3&gt;&lt;p&gt;主成分分析（PCA）是一种常见的，也是最简单的降维手段，在机器学习中可用于特征提取。即便有时收集到的样本维数很高（即含有过多特征），但与学习任务相关的可能只是某个低维分布，这时就需要有效降维，在缓解维数灾难的同时令得到的低维嵌入仍能很好地描述原样本空间。&lt;/p&gt;
    
    </summary>
    
      <category term="模式识别" scheme="http://anery.github.io/categories/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/"/>
    
    
      <category term="模式识别" scheme="http://anery.github.io/tags/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>谱聚类基本方法详解</title>
    <link href="http://anery.github.io/posts/7631c563.html"/>
    <id>http://anery.github.io/posts/7631c563.html</id>
    <published>2020-02-14T03:53:44.000Z</published>
    <updated>2020-02-16T09:15:30.834Z</updated>
    
    <content type="html"><![CDATA[<font color="#0099" size="4" face="微软雅黑">谱聚类是一种用图论思想解决聚类问题的手段。</font><a id="more"></a><h1 id="一、背景" class="heading-control"><a href="#一、背景" class="headerlink" title="一、背景"></a>一、背景<a class="heading-anchor" href="#一、背景" aria-hidden="true"></a></h1><h2 id="1-1-一些图论的知识" class="heading-control"><a href="#1-1-一些图论的知识" class="headerlink" title="1.1 一些图论的知识"></a>1.1 一些图论的知识<a class="heading-anchor" href="#1-1-一些图论的知识" aria-hidden="true"></a></h2><p>首先定义无向图$G(V,E)$的几个基本概念：</p><p>1、<strong>邻接矩阵</strong>$W$，是一个$n \times n$的对称方阵。<br>2、顶点的<strong>度矩阵</strong>$D$，是一个$n \times n$的对角矩阵，对角线元素为对应顶点的度。是由邻接矩阵各行元素累加至主对角得到的。如下图所示：</p><p><img src="https://img-blog.csdnimg.cn/20181203160433813.png" alt></p><p>当图G的边带有权重时，可将权重视为顶点间的相似度，$W$转换为相似度矩阵，顶点的度转换为连接它所有边的权重之和。<br>3、子图$A$的<strong>势</strong>$|A|$等于图的所有顶点数。<br>4、子图$A$的<strong>体积</strong> $vol(A)$ 等于所有顶点的度之和。<br>5、<strong>边割</strong>表示边的集合，去掉这些边将导致原图变成两个连通子图，如下图红边就是一个边割：</p><p><img src="https://img-blog.csdnimg.cn/20181203222736810.png" alt></p><p>6、用子图<strong>相似度</strong>来度量两个子图的相似程度，定义为连接两个子图的所有边的权重之和。显然，边割的权重之和就是它分割的两个连通子图的相似度。<br>7、<strong>最小二分切割</strong>是导致两个子图相似度最小的切割方案，又称最小代价切割。它的目标函数如下：   </p><p>​                               <img src="https://img-blog.csdnimg.cn/20181203224132268.png" alt></p><p>（从这个优化目标中已经能看出图切割任务与聚类非常相似）</p><p>通常为了防止切割出一个野点的情况，需给目标函数加上约束条件，尽量使两个子图规模相差不要太大。这叫做归一化最小二分切割。</p><p><img src="https://img-blog.csdnimg.cn/20181203223909294.png" alt></p><h2 id="1-2-拉普拉斯矩阵" class="heading-control"><a href="#1-2-拉普拉斯矩阵" class="headerlink" title="1.2 拉普拉斯矩阵"></a>1.2 拉普拉斯矩阵<a class="heading-anchor" href="#1-2-拉普拉斯矩阵" aria-hidden="true"></a></h2><p>下面引出<strong>拉普拉斯矩阵</strong>：</p><script type="math/tex; mode=display">L=D-W</script><p>由定义可知，拉普拉斯矩阵的行和为0。除此之外这个矩阵还有几个非常有用的性质：</p><ul><li><p>有1个特征值为0，它对应的特征向量元素全是1。</p><p>$L \cdot \vec1=(D-W) \cdot \vec1=\vec0=0 \cdot \vec1$</p></li></ul><ul><li><p>$L$是半正定矩阵</p><p>  <img src="https://img-blog.csdnimg.cn/20181203230613556.png" alt></p></li></ul><ul><li>$L$的特征值与图的连通分量数目的关系：设 G 为一个具有非负连接权重的无向图，它的拉普拉斯矩阵 L 零特征值的重数等于图 G 的连通子图个数 k。</li></ul><p>下面分两种情况证明第三条性质。</p><p>① k=1，即G是一个连通图，需证明的是对应的L只有一重0特征值。</p><p>设$\vec{f}$是0特征值对应的特征向量，则$L\vec{f}=0\vec{f}=\vec0$ , $\vec{f^T}L\vec{f}=\vec0$，<script type="math/tex">\sum_{i,j=1}^nw_{ij}(f_i-f_j)^2=0</script>（见上面正定二次型的证明）</p><p>所以$f_i=f_j$。这个关系将随着连通路径进行传递，也就是说特征向量$\vec{f}$的所有元素都相等，处在元素全是1的基向量张成的空间。为了满足等式，显然无法找出分量不全相等的特征向量，因此0特征值对应的特征向量只有1个。证毕。</p><p>② k&gt;1，需证明L有k重0特征值。<br>将结点按连通子图进行编号，由于不同的连通子图之间不存在边相连，因此拉普拉斯矩阵L具有分块的结构：</p><p><img src="https://img-blog.csdnimg.cn/20181203231659103.png" alt></p><p>每个$L_i$都是一个独立的拉普拉斯矩阵。由①证得的结论可知，图G有k重0特征值，它们对应的特征向量是对应连通子图节点位置元素为1，其余位置全为0的向量。</p><font color="#0099ff" size="4" face="微软雅黑">这里可以看出，如果图G的连通子图对应k个聚类，那么它的拉普拉斯矩阵0特征值对应的特征向量就可以视为聚类结果，即对应位置为1的点处在一个聚类当中。</font><p>由此引出谱聚类。</p><h1 id="二、谱聚类" class="heading-control"><a href="#二、谱聚类" class="headerlink" title="二、谱聚类"></a>二、谱聚类<a class="heading-anchor" href="#二、谱聚类" aria-hidden="true"></a></h1><p>从广义上讲，任何在学习过程中应用到<strong>矩阵特征值分解</strong>的方法都可以成为<strong>谱学习方法</strong>，比如PCA、LDA等。<strong>谱聚类</strong>算法的本质就是将聚类问题转换成图的顶点划分问题，从上面介绍的图分割目标函数的形式就可以看出这是非常相似的两个任务。了解了上述的相关图论知识后，谱聚类可以很简单地描述出来。</p><p>将聚类问题映射到图分割问题后，我们希望子图之间相似性较小，即边割权重之和尽量小，子图内部权重之和尽量大。</p><p>显然首先要做的事，也是最关键的一步，就是将我们的样本点构造成图的形式。一般分为全连接、k近邻和$\epsilon$邻域三种（后两种类似kNN、parzon窗方法）。如果数据可分性很高，就可以直接对拉普拉斯矩阵进行特征值分解，k个0特征值对应的特征向量即为聚类结果，如下图的一个例子：</p><p><img src="https://img-blog.csdnimg.cn/20181203233658737.png" alt></p><p>但事实上我们在做聚类时数据不会如此规范，一般具有黏连性，这就需要在数据稀疏的地方划出聚类边界。这种黏连性会导致拉普拉斯矩阵只有一重0特征值，无法得出聚类结果，因此可以考虑松弛情况。</p><p>假设聚类数目k已知，若没有k重0特征值，则考虑最小的k个特征值，它们更接近0。虽然它们对应的特征向量不再像0特征值的那样规整又自带分类效果，但这些向量也反映了数据本身的特征（这就是矩阵分解，也就是谱学习所带来的好处。特征值和特征向量描述了矩阵的本质特征，也就涵盖了图中样本的内在特征）。用这n个k维向量进行k-means聚类，即可得出结果。</p><p>由于最终还是应用了k-means方法，我们可以将之前的步骤看成一个<strong>表示学习、特征选择</strong>的过程，也就是用选取的特征向量代替样本点的坐标值，再进行常规的k-means距离度量。</p><p>除此之外，谱聚类还有两个扩展方法，称为Normalized Spectral Clustering。主要是变换了拉普拉斯矩阵的形式，能够达到更好的效果。这里不再详述，感兴趣的话可以去搜一下<strong>归一化图拉普拉斯</strong>。</p><h1 id="三、总结" class="heading-control"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结<a class="heading-anchor" href="#三、总结" aria-hidden="true"></a></h1><p>谱聚类利用图论的方法解决聚类问题，非常的简单直观。我觉得这种思想非常有启发性。因为我研究的是NLP方向，因此在想矩阵特征向量能否表达自然语言里的一些主要含义？最近还听说了一种图CNN算法，可以当成一种开拓思路。</p>]]></content>
    
    <summary type="html">
    
      &lt;font color=&quot;#0099&quot; size=&quot;4&quot; face=&quot;微软雅黑&quot;&gt;谱聚类是一种用图论思想解决聚类问题的手段。&lt;/font&gt;
    
    </summary>
    
      <category term="模式识别" scheme="http://anery.github.io/categories/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/"/>
    
    
      <category term="模式识别" scheme="http://anery.github.io/tags/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>贝叶斯决策类条件概率密度估计：最大似然和贝叶斯参数估计</title>
    <link href="http://anery.github.io/posts/86fca59a.html"/>
    <id>http://anery.github.io/posts/86fca59a.html</id>
    <published>2020-01-27T12:26:50.000Z</published>
    <updated>2020-01-30T12:22:02.873Z</updated>
    
    <content type="html"><![CDATA[<p><strong>有监督参数估计是指已知分类器结构或函数形式，从训练样本中估计参数。</strong>   </p><p>本文主要介绍贝叶斯决策（详见<a href="https://anery.github.io/posts/296fd81e.html">贝叶斯决策的过程</a>）条件概率密度的有监督参数估计过程。方法有最大似然估计和贝叶斯参数估计法。</p><a id="more"></a><h1 id="最大似然估计" class="heading-control"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计<a class="heading-anchor" href="#最大似然估计" aria-hidden="true"></a></h1><h2 id="假设参数为确定值，根据似然度最大进行最优估计。" class="heading-control"><a href="#假设参数为确定值，根据似然度最大进行最优估计。" class="headerlink" title="假设参数为确定值，根据似然度最大进行最优估计。"></a>假设参数为确定值，根据似然度最大进行最优估计。<a class="heading-anchor" href="#假设参数为确定值，根据似然度最大进行最优估计。" aria-hidden="true"></a></h2><p>给定数据</p><script type="math/tex; mode=display">D_1,D_2...D_c</script><p>表示不同类别的样本。假设每类样本独立同分布（i.i.d. 万年不变的假设），用$D_i$来估计$θ_i$，即对每个类求一个判别函数，用该类的样本来估计判别函数的参数。</p><p><img src="https://raw.githubusercontent.com/Anery/MyBlogPics/master/20200130145720.png#pic_center" alt> </p><p>注意区分特征空间和参数空间。参数估计的任务是得到$p(x|w_i)$的形式，是在参数空间进行的。不妨设特征空间为d维，参数空间p维。<br>为了估计参数，需要如下几个步骤：</p><ul><li><p>求似然（Likelihood）<script type="math/tex">p(D|θ) =\prod_{k=1}^{n}p(x_k|θ)</script>  注意，上面这个式子针对的已经是具体的类别$w_i$了，不要问$w$参数去哪了。另外，这里的n代表样本数目，要和前面的类别数目c区分开。这个式子很好理解，即出现我们当前观测到的样本概率，求使它最大化的参数即可。</p></li><li><p>最大化似然 <script type="math/tex">\max_θp(D|θ)→\bigtriangledown_{\theta}p(D|θ)=0</script><br>这个梯度是在p维参数空间求解，即 </p><script type="math/tex; mode=display">\bigtriangledown_{\theta}p=\begin{bmatrix}\frac{\partial}{\partialθ_1}\\...\\...\\\frac{\partial}{\partialθ_p}\end{bmatrix}</script></li><li><p>求解梯度。可求解析解或梯度下降。（常用Log-Likelihood，易求解）</p></li></ul><p>  <img src="https://raw.githubusercontent.com/Anery/MyBlogPics/master/20200130145842.png" alt></p><p>  <img src="https://raw.githubusercontent.com/Anery/MyBlogPics/master/20200130150200.png" alt></p><p>  当先验$P(\theta)$都相等时等同于最大后验概率（MAP）决策。</p><h2 id="高斯密度最大似然估计" class="heading-control"><a href="#高斯密度最大似然估计" class="headerlink" title="高斯密度最大似然估计"></a>高斯密度最大似然估计<a class="heading-anchor" href="#高斯密度最大似然估计" aria-hidden="true"></a></h2><p>以<a href="https://anery.github.io/posts/296fd81e.html">贝叶斯决策过程</a>里给出的高斯密度假设为例，对它进行最大似然参数估计。首先假设$\sigma$已知，对$\mu$进行估计。</p><p>单点情况：</p><p><img src="https://raw.githubusercontent.com/Anery/MyBlogPics/master/20200130145900.png" alt></p><p>对于所有样本：</p><p><img src="https://raw.githubusercontent.com/Anery/MyBlogPics/master/20200130145906.png" alt></p><p>估计值即为观测样本均值。</p><p>再来看$\mu$和$\sigma$都未知的情况。设数据服从一维高斯分布，$\theta_1=\mu$，$\theta_2=\sigma^2$:</p><p><img src="https://raw.githubusercontent.com/Anery/MyBlogPics/master/20200130145911.png" alt></p><p>令梯度等于0可求得：</p><script type="math/tex; mode=display">\hat{μ}=\frac{1}{n}\sum_{k=1}^nx_k</script><script type="math/tex; mode=display">\hat{σ}^2=\frac1{n}\sum_{k=1}^n(x_k-\hat{μ})^2</script><p>多维情况，$\theta_2=\Sigma$：</p><script type="math/tex; mode=display">\hat{μ}=\frac{1}{n}\sum_{k=1}^nx_k</script><script type="math/tex; mode=display">\hat{\Sigma}=\frac1{n}\sum_{k=1}^n(x_k-\hat{μ})(x_k-\hat{μ})^T</script><p>估计结果类似无偏估计。</p><h1 id="贝叶斯参数估计" class="heading-control"><a href="#贝叶斯参数估计" class="headerlink" title="贝叶斯参数估计"></a>贝叶斯参数估计<a class="heading-anchor" href="#贝叶斯参数估计" aria-hidden="true"></a></h1><h2 id="参数被视为随机变量，估计其后验分布" class="heading-control"><a href="#参数被视为随机变量，估计其后验分布" class="headerlink" title="参数被视为随机变量，估计其后验分布"></a>参数被视为随机变量，估计其后验分布<a class="heading-anchor" href="#参数被视为随机变量，估计其后验分布" aria-hidden="true"></a></h2><p>我们先来简化一下贝叶斯决策的条件概率密度形式。考虑训练样本对分类决策的影响，后验概率可写作：</p><p><img src="https://raw.githubusercontent.com/Anery/MyBlogPics/master/20200130145917.png" alt></p><p>首先由于先验概率一般可以事先得到，因此通常不考虑样本对它的影响。其次，我们使用的是有监督学习，训练样本自然都会分到各自所属的类中。基于这两点可简化公式，得到<strong>公式一</strong>：</p><p><img src="https://raw.githubusercontent.com/Anery/MyBlogPics/master/20200130145922.png" alt></p><p>由此我们需处理的其实是c个独立的问题，那么条件概率密度可简写成c个$P(x|D)$，分别对它们进行估计。</p><p>下面引出参数分布估计的过程。假定参数形式已知，即已知$p(x|θ)$，为求$p(x|D)$:</p><script type="math/tex; mode=display">p(x|D)=\int{p(x,θ|D)}dθ \\  \qquad\qquad \qquad=\int{p(x|θ,D)p(θ|D)dθ}</script><p>由于测试样本x（观测样本）和训练样本D的选取是独立的，因此可写成<strong>公式二</strong>：</p><script type="math/tex; mode=display">\quad p(x|D)=\int{p(x|θ)p(θ|D)dθ}</script><p>样本独立性是《模式分类第二版》里对这步变换做出的解释。对这一部分说一下我的理解。按书里说的x与D相互独立，那p(x|D)其实直接就可以简写成p(x)，且$p(\theta)$也假定已知（后面会说），直接</p><script type="math/tex; mode=display">\quad p(x)=\int{p(x|θ)p(θ)dθ}</script><p>不就能求了，为什么非要对条件概率密度引入D呢？</p><p>其实这样做的目的就是为了<strong>强行引入</strong>$p(\theta|D)$。别忘了$p(x|D)$实际上是$p(x|\omega,D)$，来自<strong>公式一</strong>。回顾一下公式一引入D的原因，是<strong>尽可能地利用已有的全部信息来估计后验概率$p(\omega|x)$</strong>，对$p(x|D)$也是这样。即便训练样本对观测值x没有影响，但我们希望再引入一个受样本影响的reproducing density $p(\theta|D)$，让它影响类条件概率的分布。其实相当于重新构造了一个先验，并希望$p(\theta|D)$在$\theta$的真实值附近有显著的尖峰（sharp）。通常可以用这个sharp逼近的$\hat\theta$来替代真实值，有$p(x|D) ≈ p(x|\hat\theta)$。如果估计值的置信度不高（用高斯分布来说即方差较大，sharp不明显。后面会说），也可以按$p(\theta|D)$对$\theta$进行采样，带入$p(x|\theta)$求平均：</p><p><img src="https://raw.githubusercontent.com/Anery/MyBlogPics/master/20200130145928.png" alt></p><p>总结一下，<strong>公式一</strong>和<strong>公式二</strong>是贝叶斯决策和参数估计的两个核心部分。尤其是<strong>公式二</strong>，我们希望把$p(x|D)$和$p(θ|D)$联系起来，那么已有的训练样本就能通过$p(θ|D)$对$p(x|D)$施加影响。<strong>至此我们已经把有监督学习问题（原始分类问题）转换成了一个无监督的概率密度预测问题（估计$p(θ|D)$）</strong>。</p><h2 id="高斯密度贝叶斯估计" class="heading-control"><a href="#高斯密度贝叶斯估计" class="headerlink" title="高斯密度贝叶斯估计"></a>高斯密度贝叶斯估计<a class="heading-anchor" href="#高斯密度贝叶斯估计" aria-hidden="true"></a></h2><p>对高斯密度假设进行贝叶斯参数估计。</p><p>考虑一维情况。$p(x|\mu)\sim N(μ，σ^2)$，假设$σ^2$已知，为了预测$p(μ|D)$，写成：</p><script type="math/tex; mode=display">p(μ|D)=\frac{p(D|μ)p(μ)}{\int{p(D|μ)p(μ)dμ}}</script><p>由于$p(D|\mu)=\prod_{k=1}^np(x_k|μ)$，则</p><script type="math/tex; mode=display">p(μ|D)=\alpha\prod_{k=1}^np(x_k|μ)p(μ)</script><p>$\alpha$是原式分母，作为常数项。</p><p>假设$p(μ)\sim N(μ_0，σ_0^2)$，$\mu_0$和$\sigma_0^2$已知。可以把$\mu_0$看作对$\mu$的先验估计，$\sigma_0^2$看作估计的不确定程度。做正态分布假设只是为了简化后面的数学运算。这一步的重点在于<strong>在参数估计过程中我们是已知参数先验概率密度$p(\mu)$的。</strong></p><p>公式展开：</p><p><img src="https://raw.githubusercontent.com/Anery/MyBlogPics/master/20200130145937.png" alt></p><p>与μ无关的因子都被归入$\alpha$中。可见$p(μ|D)$仍符合高斯分布，对照标准形式<script type="math/tex">p(μ|D)=\frac{1}{\sqrt{2\pi}σ_n}exp(-\frac{1}{2}\frac{(\mu-μ_n)^2}{σ_n^2})</script>可得</p><p><img src="https://raw.githubusercontent.com/Anery/MyBlogPics/master/20200130145944.png" alt></p><p>到目前为止，已经把先验知识$p(\mu)$和训练样本信息$\hat\mu_n$结合在一起，估计出了后验概率$p(\mu|D)$。把结果直观地写在一起：</p><p><img src="https://raw.githubusercontent.com/Anery/MyBlogPics/master/20200130145949.png" alt></p><p>在这个结果中，$\mu_n$表示在观测到n个样本后，对参数$\mu$真实值的最好估计，$\sigma_n^2$则代表这个估计的不确定性（前面对先验假设也是这么解释的，理解一下高斯分布对参数估计的理论意义）。$\sigma_n^2$随着n的增大而减小，即增加训练样本后，对$\mu$真实估计的置信度将逐渐提高，呈现一个<strong>sharp</strong>。这样的过程称为贝叶斯学习过程。</p><p>将$p(\mu|D)$代入</p><script type="math/tex; mode=display">p(x|D)=\int{p(x|μ)p(μ|D)dμ}</script><p>得出$p(x|D)\sim{N(μ_n，σ^2+σ_n^2)}$。因此，根据已知的$p(x|μ)\sim{N(μ，σ^2)}$，只要用$μ_n$替换μ，$σ^2+σ_n^2$替换$σ^2$即可完成参数估计。</p><p>我们观察到，当n趋于无穷时，贝叶斯参数估计与最大似然效果相同。（当然在实际问题当中样本往往是有限的，这里只是形式化地理解）</p><p>总结一下贝叶斯估计的一般过程：</p><p>  <img src="https://raw.githubusercontent.com/Anery/MyBlogPics/master/20200130145954.png" alt></p><h1 id="最大似然和贝叶斯估计的比较" class="heading-control"><a href="#最大似然和贝叶斯估计的比较" class="headerlink" title="最大似然和贝叶斯估计的比较"></a>最大似然和贝叶斯估计的比较<a class="heading-anchor" href="#最大似然和贝叶斯估计的比较" aria-hidden="true"></a></h1><p>在上面的例子中，用贝叶斯参数估计与ML分别对条件概率密度$p(x|\omega)$进行估计，得到的虽然都是高斯分布形式，但这个过程中做的假设是完全不同的。ML直接假定$p(x|\omega)$符合高斯分布，根据训练样本选取确定的参数$\hat\mu$和$\hat\sigma^2$。而贝叶斯估计方法是通过假设已知$p(x|θ)$和$p(\mu)$符合高斯分布，推出$p(\mu|D)$符合高斯分布， 进而根据<strong>公式二</strong>推出$p(x|D)$符合高斯分布。这个分布的sharp作为估计的均值，随样本数增加而改变，且确信度逐渐升高。</p><p>高斯分布的例子相对来说有点抽象，《模式分类》里还给了一个简单的例子，比较好理解，尤其是这幅图：</p><p><img src="https://raw.githubusercontent.com/Anery/MyBlogPics/master/20200130150032.png" alt></p><p>非常有助于理解。贝叶斯估计在样本最大值之外还有一个拖尾，这就是考虑了先验$p(\theta)$的结果，告诉我们在x=10附近，条件概率密度仍可能不为0。<em>（详见书中例1 递归的贝叶斯学习）</em></p><p>总的来说，最大似然估计根据训练样本明确估计出最优参数值，而贝叶斯估计目标是求出参数的分布，类似于“参数为0.5的概率为0.8”。虽然在估计时模糊的结果（即近似正确）往往更有用，但贝叶斯估计计算复杂度较高，可理解性较差，因此最大似然估计应用更广泛。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;有监督参数估计是指已知分类器结构或函数形式，从训练样本中估计参数。&lt;/strong&gt;   &lt;/p&gt;
&lt;p&gt;本文主要介绍贝叶斯决策（详见&lt;a href=&quot;https://anery.github.io/posts/296fd81e.html&quot;&gt;贝叶斯决策的过程&lt;/a&gt;）条件概率密度的有监督参数估计过程。方法有最大似然估计和贝叶斯参数估计法。&lt;/p&gt;
    
    </summary>
    
      <category term="模式识别" scheme="http://anery.github.io/categories/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/"/>
    
    
      <category term="模式识别" scheme="http://anery.github.io/tags/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>深度神经网络排错实践指南</title>
    <link href="http://anery.github.io/posts/48f17f8a.html"/>
    <id>http://anery.github.io/posts/48f17f8a.html</id>
    <published>2019-08-04T09:16:04.000Z</published>
    <updated>2020-02-16T08:39:06.717Z</updated>
    
    <content type="html"><![CDATA[<p>深度学习排错指南，主要内容翻译自<a href="http://t.cn/EtUAfzl" target="_blank" rel="noopener">此PPT</a>，选取了自己认为有用的部分记录。</p><a id="more"></a><h3 id="模型表现差的原因：" class="heading-control"><a href="#模型表现差的原因：" class="headerlink" title="模型表现差的原因："></a>模型表现差的原因：<a class="heading-anchor" href="#模型表现差的原因：" aria-hidden="true"></a></h3><ul><li>implementation bugs</li><li>hyperparameter choices</li><li>Data/model fit 实验数据质量</li><li>dataset construction: 数据不够，类别不均衡，标签噪声，训练和测试集分布不同</li></ul><h3 id="debug困难原因：" class="heading-control"><a href="#debug困难原因：" class="headerlink" title="debug困难原因："></a>debug困难原因：<a class="heading-anchor" href="#debug困难原因：" aria-hidden="true"></a></h3><ul><li>很难知道是否有bug</li><li>错误来源很多</li><li>结果对超参数和数据集组成的小改变很敏感</li></ul><h3 id="troubleshooting策略：" class="heading-control"><a href="#troubleshooting策略：" class="headerlink" title="troubleshooting策略："></a>troubleshooting策略：<a class="heading-anchor" href="#troubleshooting策略：" aria-hidden="true"></a></h3><p><img src="https://raw.githubusercontent.com/Anery/MyBlogPics/master/1561450332641.png" alt></p><ul><li>start simple： 选择最简单的模型和数据</li><li>implement&amp;debug：若能跑起来，试图在一个批次上过拟合 &amp; 重现一个简单、已知的结果（关于过拟合后面详述）</li><li>evaluate：用bias-variance分解来决定下一步</li><li>tune hyper-parameters：用由粗到细随机搜索</li><li>improve model/data：若欠拟合，让模型变大；若过拟合，增加数据和正则项</li></ul><p>下面用例子来一步步说明。</p><p>假设你已经有：</p><ul><li>初始测试集</li><li>一个待提升的评估指标</li><li>基于human-level performance的目标表现，公开结果，先前的baseline，等等。</li></ul><p>比如：</p><p><img src="https://raw.githubusercontent.com/Anery/MyBlogPics/master/1561451044291.png" alt></p><ol><li><p><strong>Starting simple</strong></p><ul><li><p>先选一个简单的结构：</p><p>| 你的输入数据 |      Start       |       下一步考虑       |<br>| :—————: | :———————: | :——————————: |<br>|     图像     |      LeNet       |         ResNet         |<br>|   文本序列   | 一层隐藏层的LSTM | Attention模型或WaveNet |<br>|     其它     | 一层隐藏层的FCN  |       视问题而定       |</p><p>特殊情形：多种输入模式，如image caption</p></li><li><p>用合理的默认值</p><ul><li><p>Optimizer：Adam，学习率3e-4</p></li><li><p>Activations：ReLu（FC和CNN），tanh（LSTMs）</p></li><li><p>Initialization：He et al. normal (used for relu) , Glorot normal (used for tanh) （Glorot normal即为Xavier初始化，pytorch里有，或TensorFlow里的 tf.glorot_normal_initializer ）</p><p><img src="https://raw.githubusercontent.com/Anery/MyBlogPics/master/1561452521433.png" alt></p></li><li><p>Regulazation：无（可不用）</p></li><li><p>Data normalization：无</p></li></ul></li><li><p>输入归一化：减均值除方差</p></li><li><p>简化问题：</p><ul><li>从一个小训练集开始（&lt;10000个实例）</li><li>用固定的objects数量，类数量，更小的图像尺寸，等等</li><li>创建一个更简单的人工合成训练集</li></ul></li></ul><p>针对前面那个例子就是：</p><p><img src="https://raw.githubusercontent.com/Anery/MyBlogPics/master/1561453222714.png" alt></p></li><li><p><strong>Implement &amp; debug</strong></p><p>==<strong>在展开说这部分之前，作者总结了5个最常见的DL bug，可以先逐个排查：</strong>==</p><p>（1）tensor的shape出错（深表同感），直接报错shape错误就不说了，相对容易排查，需要注意的是能跑起来的错误，一般由广播策略引起。如 x.shape = (None,) , y.shape = (None,1)，(x+y).shape = (None,None)</p><p>（2）预处理输入出错（原文：Pre-processing inputs incorrectly，有点没理解）。比如，忘记归一化，或预处理过多</p><p>（3）损失函数接收的输入出错。比如，对期望收到logits的损失函数输入了softmax结果。</p><p>（4）忘记设置train/eval mode。有些策略在train和test时有不同的实现。</p><p>（5）数值不稳定，出现Inf/nan。通常源于使用exp、log或div操作。</p><p>==<strong>在实现模型时的一些建议：</strong>==</p><p>（1）轻量级实现。少加新代码，实现少于200行（经验）</p><p>（2）使用封装好的组件，如Keras。在具体实现上，使用<code>tf.layers.dense(…)</code> 替代 <code>tf.nn.relu(tf.matmul(W, x))</code>，使用<code>tf.losses.cross_entropy(…)</code>替代具体实现。（我个人认为在学习阶段更重要的是快速实现想法和按自己的需求改模型，所以Keras和pytorch可能是更好的选择。）</p><p>（3）以后再学习构造复杂的data生成pipeline。先从可以全部load到内存的数据开始。</p></li></ol><ul><li><p>让模型跑起来</p><p>常见错误：</p><ul><li><p>shape不匹配。</p><p><strong>常见原因：</strong>（1）sum,average,softmax操作在错误的维度（2）卷积层后忘记展平tensor（3）忘记去掉多余的“1“维度，如 (1,1,4) （4）存在磁盘上的数据类型与load时不符，比如存了个float64的numpy array，load的是float32。</p><p><strong>解决：</strong>开debugger，逐步进行模型创建和测试</p></li><li><p>数据类型错误。</p></li><li><p>OOM。</p><p><strong>常见原因：</strong>（1）tensor太大。一般是因为（evaluation时）batch size过大，或庞大的全连接层。（2）数据太多。load太多数据到内存，而不是用输入队列；或为创建数据集分配过大的buffer。（3）冗余的操作。可能在一个Session里创建了太多模型，或重复调用某个操作。</p><p>（4）有人占了你的卡（其实这条最实用）</p><p><strong>解决：</strong>逐项删掉占用内存密集的操作。</p></li><li><p>其它。</p><p><strong>常见原因：</strong> 忘记初始化变量；Forgot to turn off bias when using batch norm（这条不清楚什么意思？）</p><p><strong>解决：</strong>使用标准debugg工具包，如stack overflow和交互式debugger</p></li></ul><p>不同的框架有不同的debugger，pytorch的ipdb比较简单，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> ipdb; ipdb.set_trace()</span><br></pre></td></tr></table></figure><p>作者给出的两种使用方法：</p><p><img src="https://raw.githubusercontent.com/Anery/MyBlogPics/master/1561628632132.png" alt></p><p><img src="https://raw.githubusercontent.com/Anery/MyBlogPics/master/1561628644818.png" alt></p><p>不是很明确，用到的时候再搜一下用法。</p></li></ul><ul><li><p>在一个batch上过拟合</p><p>常见错误：</p><ul><li><p>error上升</p><p><strong>常见原因：</strong>（1）loss function或梯度的符号反了    （2）学习率太高    （3）softmax时对应的维度出错</p></li><li><p>error爆炸</p><p><strong>常见原因：</strong>（1）数值问题，检查exp、log和div操作    （2）学习率太高</p></li><li><p>error震荡</p><p><strong>常见原因：</strong>（1）数据或标签出错，如zeroed，错误shuffled，预处理出错    （2）学习率太高</p></li><li><p>error平稳</p><p><strong>常见原因：</strong>（1）学习率太低    （2）梯度没有传递给整个模型（梯度消失）    （3）太多正则化    （4）loss function的输入出错（比如，输入了softmax而不是logits）    （5）数据或标签出错</p></li></ul></li><li><p>与已知结果比较</p><p>作用由高到低：</p><ul><li><p>在你用的数据集上有官方模型实现</p><p>可按行输出中间结果，看是否有相同输出。</p></li><li><p>在benchmark上有官方模型实现</p><p>可按行输出中间结果，看是否有相同输出。</p></li><li><p>非官方模型实现</p></li><li><p>paper结果（只能对比预期指标）</p></li></ul></li></ul><ol><li><p><strong>Evaluate</strong></p><p>一般认为：<strong>Test error = irreducible error（无法消除的误差项） + bias + variance + val overfitting</strong></p><p>但前提是训练、验证、测试数据服从相同分布，如果出现下面这种情况：</p><p><img src="https://raw.githubusercontent.com/Anery/MyBlogPics/master/1563710877694.png" alt></p><p>称为分布偏差（distribution shift），由于验证集一般是从训练集中选择的，因此与训练集具有同分布。此时验证集的结果并没有泛化到测试集的能力。</p><p>此时：<strong>Test error = irreducible error + bias + variance + ==distribution shift== + val overfitting</strong></p></li><li><p><strong>Improve model/data</strong></p><p>优先考虑bias-variance tradeoff：解决欠拟合、解决过拟合、解决distribution shift、平衡数据集</p><ul><li>解决欠拟合（降低偏差）<ul><li>让模型变得“更大”，横向增加hidden size，纵向增加layers</li><li>减少正则项</li><li>误差分析</li><li>选择不同的模型结构（如从LeNet转到ResNet）</li><li>调整超参</li><li>增加features</li></ul></li><li>解决过拟合（降低方差）<ul><li>增加训练数据（可能的话）</li><li>增加normalization（如batch norm，layer norm）</li><li>数据增强（data augmentation）</li><li>增加正则项（dropout，L2，权重衰减）</li></ul></li><li>解决distribution shift（优先级从高到低）<ul><li>分析test-val set的误差，收集更多训练数据来补偿</li><li>分析test-val set的误差，合成更多训练数据来补偿</li><li>将领域适应技术应用于训练集和测试集分布（Apply domain adaptation techniques to training &amp; test distributions）。什么是domain adaptation？就是在源分布上训练完后，只用无标注数据或有限的标注数据将模型泛化到目标分布。可以理解为迁移学习。</li></ul></li><li>平衡数据集</li></ul></li><li><p><strong>Tune hyperparameters</strong></p><p>经验法则：</p><p><img src="https://raw.githubusercontent.com/Anery/MyBlogPics/master/1563876305090.png" alt></p><ul><li><p>人工调参</p><p>首先要理解算法，理解每个超参的作用。比如，更高的learning rate意味着学得更快，但会降低学习的稳定性。其次就是不断训练和验证模型，用经验去“猜”更好的超参数，或手动选择参数范围。</p></li><li><p>grid search</p><p>自动枚举各种参数组合。但效率很低。</p></li><li><p>随机搜索</p><p>随机设置参数组合，选择表现好的一个区域，再次随机搜索，缩小范围。一般可以选到很好的超参数，<strong>实际中最常用。</strong></p></li><li><p>贝叶斯超参数优化</p></li></ul></li></ol><h3 id="结论" class="heading-control"><a href="#结论" class="headerlink" title="结论"></a>结论<a class="heading-anchor" href="#结论" aria-hidden="true"></a></h3><ul><li>DL debugger很难，因为错误来源非常广</li><li>为了得到bug-free的模型，我们应当把模型构建过程当作一个迭代过程</li></ul><h3 id="learn-more" class="heading-control"><a href="#learn-more" class="headerlink" title="learn more"></a>learn more<a class="heading-anchor" href="#learn-more" aria-hidden="true"></a></h3><ul><li>Andrew Ng’s book Machine Learning Yearning (<a href="http://www.mlyearning.org/" target="_blank" rel="noopener">http://www.mlyearning.org/</a>)</li><li>The following Twitter thread:  <a href="https://twitter.com/karpathy/status/1013244313327681536" target="_blank" rel="noopener">https://twitter.com/karpathy/status/1013244313327681536</a></li><li>This blog post:  <a href="https://pcc.cs.byu.edu/2017/10/02/practical-advice-for-building-deep-neuralnetworks/" target="_blank" rel="noopener">https://pcc.cs.byu.edu/2017/10/02/practical-advice-for-building-deep-neuralnetworks/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;深度学习排错指南，主要内容翻译自&lt;a href=&quot;http://t.cn/EtUAfzl&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;此PPT&lt;/a&gt;，选取了自己认为有用的部分记录。&lt;/p&gt;
    
    </summary>
    
      <category term="deep learning" scheme="http://anery.github.io/categories/deep-learning/"/>
    
    
      <category term="技巧" scheme="http://anery.github.io/tags/%E6%8A%80%E5%B7%A7/"/>
    
  </entry>
  
  <entry>
    <title>贝叶斯决策的过程</title>
    <link href="http://anery.github.io/posts/296fd81e.html"/>
    <id>http://anery.github.io/posts/296fd81e.html</id>
    <published>2019-03-31T15:07:42.000Z</published>
    <updated>2020-01-30T12:23:32.572Z</updated>
    
    <content type="html"><![CDATA[<h1 id="最小风险决策" class="heading-control"><a href="#最小风险决策" class="headerlink" title="最小风险决策"></a>最小风险决策<a class="heading-anchor" href="#最小风险决策" aria-hidden="true"></a></h1><p>最小风险决策是贝叶斯决策的一般形式。引入决策代价loss：<script type="math/tex">\lambda_{ij}=\lambda (\alpha_i|\omega_j)</script> ，表示原本属于类 $j$，被错分为类 $i$ 所产生的风险（BTW，<script type="math/tex">\lambda_{ij}</script> 与 <script type="math/tex">\lambda_{ji}</script> 并不相等，有时相差很大。比如肿瘤检测时)。则条件风险 <script type="math/tex">R(\alpha_i|x) = \sum_{j=1}^{c}\lambda(\alpha_i|\omega_j)P(\omega_j|x)</script> 贝叶斯决策就要选择最小化该条件风险的类别 $i$。</p><p>当 $\lambda$ 为0/1损失时，$R(\alpha_i|x) = 1 - P(\omega_i|x)$，最小风险决策退化为最小错误率决策，或最大后验决策。</p><a id="more"></a><h1 id="带拒识的决策" class="heading-control"><a href="#带拒识的决策" class="headerlink" title="带拒识的决策"></a>带拒识的决策<a class="heading-anchor" href="#带拒识的决策" aria-hidden="true"></a></h1><p>在很多模式识别应用中，当最大后验也不是很高，也就是置信度低的情况下，很可能出现了不可分情况，可让分类器拒绝分类。设计拒识风险 $\lambda_r$，表示为：</p><p>​                                <img src="https://img-blog.csdnimg.cn/20181227202031326.png" alt></p><p>则风险$R$表示为：</p><p>​                              <img src="https://img-blog.csdnimg.cn/20181227202242274.png" alt></p><p>可知当$\lambda_s[1 - P(\omega_i|x)] &gt; \lambda_r$时选择拒识，决策方式写作：</p><p>​                 <img src="https://img-blog.csdnimg.cn/20181227202640209.png" alt></p><p>这种写法比较形式化，可以简单的理解$\max P(\omega_i|x)$小于某个阈值时视为置信度过低，拒绝分类。</p><h1 id="贝叶斯决策具体过程" class="heading-control"><a href="#贝叶斯决策具体过程" class="headerlink" title="贝叶斯决策具体过程"></a>贝叶斯决策具体过程<a class="heading-anchor" href="#贝叶斯决策具体过程" aria-hidden="true"></a></h1><p>前面定义了贝叶斯决策的风险函数，主要与类别后验相关。根据贝叶斯公式$P(\omega_i|x) = \frac{P(x|\omega_i)P(\omega_i)}{P(x)}$，给出贝叶斯决策的几种判别函数：</p><p>​                         $g_i(x) = P(\omega_i|x)$ ，$g_i(x) = P(x|\omega_i)P(\omega_i)$，$g_i(x) = -R(\alpha_i|x)$</p><p>还有一种常用的似然形式：$g_i(x) = \log P(x|\omega_i) + \log P(\omega_i)$</p><p>则判别决策为$\arg\max g_i(x) $。令判别函数相等可得到决策面，能够在几何上对特征空间进行分割，让我们对分类有直观的认识，加深对分类器的理解。</p><p>贝叶斯决策几个关键步骤：</p><blockquote><p>1、估计类条件概率密度$P(x|\omega_i)$</p><p>2、估计类先验概率$P(\omega_i)$   (一般从训练数据中统计)</p><p>3、决策代价$\lambda_{ij}$。（除非面向特定应用，否则一般用0/1损失，即最大后验决策）</p></blockquote><h2 id="类条件概率密度估计方法" class="heading-control"><a href="#类条件概率密度估计方法" class="headerlink" title="类条件概率密度估计方法"></a>类条件概率密度估计方法<a class="heading-anchor" href="#类条件概率密度估计方法" aria-hidden="true"></a></h2><blockquote><p>1、<strong>参数法。</strong>假定概率密度函数形式，如高斯分布。</p><p>2、<strong>非参数法</strong>。如Parzen窗，k-NN。</p><p>3、<strong>半参数法。</strong>高斯混合。</p></blockquote><h3 id="高斯密度函数法" class="heading-control"><a href="#高斯密度函数法" class="headerlink" title="高斯密度函数法"></a>高斯密度函数法<a class="heading-anchor" href="#高斯密度函数法" aria-hidden="true"></a></h3><p>假定类条件概率密度符合高斯分布，那么只要估计出均值和协方差矩阵即可得到$p(x|\omega_i)$。参数估计过程详见<a href="https://anery.github.io/posts/86fca59a.html">最大似然和贝叶斯参数估计</a>。</p><p>将条件概率分布写成多元高斯形式:</p><p>​                         <img src="https://img-blog.csdnimg.cn/2018122817162463.png" alt></p><p>得到似然形式的判别函数$g_i(x)$：</p><p>​                 <img src="https://img-blog.csdnimg.cn/20181228171706393.png" alt></p><p>在数据的不同分布情况下，我们能得到一些特殊的形式。下面根据协方差矩阵的3种形式（逐渐推广），直观地来看一下数据分布、先验等对贝叶斯决策的影响。</p><ul><li><p><strong>Case 1</strong>  <script type="math/tex">\Sigma_i =\sigma^2</script></p><p>  协方差矩阵是对角矩阵，且对角元素相等。这种情况说明数据在各个维度（或说特征，对应特征空间）上的分布独立，且各个维度的方差均为<script type="math/tex">\sigma^2</script>。从几何上来说，所有类的样本分别落在一个形状相同的超球体当中。判别函数用于分类，因此只考虑类别相关的部分，即与i相关的项，其余部分不考虑，判别函数写成：</p><p>  <img src="https://img-blog.csdnimg.cn/20181228172944726.png" alt></p><p>  将二范数平方展开，得到</p><p>  <img src="https://img-blog.csdnimg.cn/20181228173017703.png" alt></p><p>  注意到<script type="math/tex">x^tx</script>与类别无关，可不考虑。为了直观地获得一般形式的决策超平面，我们将判别函数写成线性形式：</p><p>  <img src="https://img-blog.csdnimg.cn/20181228173209664.png" alt></p><p>  以二分类情况为例，令<script type="math/tex">g_i(x)-g_j(x)=0</script>即可得到决策面，表示为：</p><p>  <img src="https://img-blog.csdnimg.cn/2018122817323655.png" alt><br>  <img src="https://img-blog.csdnimg.cn/20181228173245467.png" alt></p><p>  我们观察到，法向量等于两类类心点的差，因此<strong>决策面与两个中心点连线垂直</strong>。决策面与这条直线相交于点<script type="math/tex">x_0</script>。</p><p>  再来观察<script type="math/tex">x_0</script>，当先验概率相等即<script type="math/tex">P(\omega_i)=P(\omega_j)</script>时，<script type="math/tex">x_0</script>第二项为0，那么该点落在两中心点连线的中心，如图（图来自《模式分类第二版》）</p><p>  <img src="https://img-blog.csdnimg.cn/20181228173826942.png" alt></p><p>  当先验不相等时，决策面将随着点<script type="math/tex">x_0</script>向先验概率小的方向偏移，如下图</p><p>  <img src="https://img-blog.csdnimg.cn/20181228173917124.png" alt></p><p>  这里给出的是比较极端的一种情况，先验相差较大，<script type="math/tex">P(\omega_1)=0.9,P(\omega_2)=0.1</script>。这就使得决策面偏移很大，甚至没有落在两个均值向量之间，可见<strong>先验概率对分类结果也有很大的影响。当然，当<script type="math/tex">\sigma^2</script>远小于<script type="math/tex">||\mu_i-\mu_j||^2</script>时，<script type="math/tex">x_0</script>第二项将变得很小，使决策面对先验不再敏感。</strong></p></li><li><p><strong>Case 2</strong>  <script type="math/tex">\Sigma_i =\Sigma</script></p><p>各个类的协方差矩阵相同，但均值各不相同。这显然是Case 1的一个推广。</p><p>首先同样只保留类别相关项，即</p><p>​                          <img src="https://img-blog.csdnimg.cn/20181228174726102.png" alt></p><p>将判别函数按照与刚才同样的方法转换成一般形：</p><p>​               <img src="https://img-blog.csdnimg.cn/20181228174850582.png" alt><br>​         <img src="https://img-blog.csdnimg.cn/20181228174858387.png" alt="img"></p><p>再得到决策面：</p><p>​                           <img src="https://img-blog.csdnimg.cn/20181228174822129.png" alt></p><p>​                       <img src="https://img-blog.csdnimg.cn/20181228174835112.png" alt></p><p>我们再来看这种形式。首先，法向量经一个矩阵变换，使得决策面不再垂直于两中心点连线，但仍与直线交于点$x_0$。当先验概率相等时，$x_0$位于连线中点，否则依然偏向先验小的一类。如下图所示。</p><p>​                        <img src="https://img-blog.csdnimg.cn/20181228171607649.png" alt></p><p>由于协方差矩阵不再是对角矩阵，各个维度相互依赖，使得样本分布的形状呈椭球形，而不再是标准的球形。但由于各类协方差矩阵相同，因此它们的分布形状都是一样的。可以观察一下，<strong>决策面的方向其实与椭球最长轴的方向一致，这与协方差矩阵的几何性质有关，主成分分析（PCA）也利用了这一点。</strong></p></li><li><p><strong>Case 3</strong>  <script type="math/tex">\Sigma_i</script>任意</p><p>这是最一般的情况，各个类的样本分布形状各不相同，由贝叶斯判别边界求出的决策面也不再是超平面，而成为了超曲面。</p><p>《模式分类第二版》里给出了几个直观的2D情况的例子：</p><p>​                <img src="https://img-blog.csdnimg.cn/20181228180649875.png" alt></p><p>总结一下，列出这三种情况其实就是为了说明数据分布情况对高斯情况下决策的影响。<strong>这些图也帮助我们理解了协方差矩阵是如何反映数据分布的</strong>。</p></li></ul><h3 id="分类错误率" class="heading-control"><a href="#分类错误率" class="headerlink" title="分类错误率"></a>分类错误率<a class="heading-anchor" href="#分类错误率" aria-hidden="true"></a></h3><p>考虑二分类，决策面把特征空间分成$R_1$和$R_2$两个部分，那么错误分类的情况发生在真实类为$\omega_1$，观测值落在R2中，或真实类为$\omega_2$，观测值落在$R_1$中时。形式化表示为：</p><p>​                     <img src="https://img-blog.csdnimg.cn/20181228183104960.png" alt></p><p>推广到一般形式，计算正确率更为方便：</p><p>​                          <img src="https://img-blog.csdnimg.cn/20181228183452415.png" alt></p><p>再来考虑最大后验概率决策，即0/1损失的情况：</p><p>​                      <img src="https://img-blog.csdnimg.cn/20181228183614756.png" alt></p><h1 id="总结" class="heading-control"><a href="#总结" class="headerlink" title="总结"></a>总结<a class="heading-anchor" href="#总结" aria-hidden="true"></a></h1><p>贝叶斯分类器使用的是最小错误率决策，理论情况下，即条件概率密度函数和风险$\lambda$被正确估计时是最优分类器。然而通常我们都仅仅是假设了条件概率密度函数的形式，如前面所述的高斯分布，导致效果出现偏差。事实上这里有两层误差，一个是结构误差，即对真实样本分布的错误估计，另一个是模型参数估计误差。这两层误差是造成贝叶斯分类器效果不好的原因，而并非分类器本身的缺陷。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;最小风险决策&quot;&gt;&lt;a href=&quot;#最小风险决策&quot; class=&quot;headerlink&quot; title=&quot;最小风险决策&quot;&gt;&lt;/a&gt;最小风险决策&lt;/h1&gt;&lt;p&gt;最小风险决策是贝叶斯决策的一般形式。引入决策代价loss：&lt;script type=&quot;math/tex&quot;&gt;\lambda_{ij}=\lambda (\alpha_i|\omega_j)&lt;/script&gt; ，表示原本属于类 $j$，被错分为类 $i$ 所产生的风险（BTW，&lt;script type=&quot;math/tex&quot;&gt;\lambda_{ij}&lt;/script&gt; 与 &lt;script type=&quot;math/tex&quot;&gt;\lambda_{ji}&lt;/script&gt; 并不相等，有时相差很大。比如肿瘤检测时)。则条件风险 &lt;script type=&quot;math/tex&quot;&gt;R(\alpha_i|x) = \sum_{j=1}^{c}\lambda(\alpha_i|\omega_j)P(\omega_j|x)&lt;/script&gt; 贝叶斯决策就要选择最小化该条件风险的类别 $i$。&lt;/p&gt;
&lt;p&gt;当 $\lambda$ 为0/1损失时，$R(\alpha_i|x) = 1 - P(\omega_i|x)$，最小风险决策退化为最小错误率决策，或最大后验决策。&lt;/p&gt;
    
    </summary>
    
      <category term="模式识别" scheme="http://anery.github.io/categories/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/"/>
    
    
      <category term="模式识别" scheme="http://anery.github.io/tags/%E6%A8%A1%E5%BC%8F%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
</feed>
